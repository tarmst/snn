{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71c9cf63-4b8b-4843-a766-a3fe2740189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tonic\n",
    "import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e804e18c-22f4-452f-9c6b-f688496af46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tonic.datasets.NMNIST(save_to='./data/tonic', train=True)\n",
    "events, target = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa307682-6f93-4581-915c-7462d2d2e273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 34, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "sensor_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed9548b5-90ea-4247-82f7-954c90025214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tonic.utils.plot_event_grid(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "444cd206-49f8-4ed1-ac81-c190a8a3cb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com/a99d0fee-a95b-4231-ad22-988fdb0a2411 to ./data/tonic/NMNIST/test.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad65f9ddb79841d9a99e3274ffbf10e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/169674850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/tonic/NMNIST/test.zip to ./data/tonic/NMNIST\n"
     ]
    }
   ],
   "source": [
    "import tonic.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "# Denoise removes isolated, one-off events\n",
    "# time_window\n",
    "frame_transform = transforms.Compose([transforms.Denoise(filter_time=10000),\n",
    "                                      transforms.ToFrame(sensor_size=sensor_size,\n",
    "                                                         time_window=1000)\n",
    "                                     ])\n",
    "\n",
    "trainset = tonic.datasets.NMNIST(save_to='./data/tonic', transform=frame_transform, train=True)\n",
    "testset = tonic.datasets.NMNIST(save_to='./data/tonic', transform=frame_transform, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d20cb84-406f-40df-a317-47ca0fc994f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47902679-8cef-4312-b8bb-88a0577ffd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tonic import DiskCachedDataset\n",
    "\n",
    "\n",
    "cached_trainset = DiskCachedDataset(trainset, cache_path='./cache/nmnist/train')\n",
    "cached_dataloader = DataLoader(cached_trainset)\n",
    "\n",
    "batch_size = 128\n",
    "trainloader = DataLoader(cached_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29647122-c483-4655-8756-a518e6c7da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_batched():\n",
    "    events, target = next(iter(cached_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3e564af-8cf6-4b4d-bc5e-a90391e5057c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9 ms ± 84.4 µs per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TimeitResult : 3.9 ms ± 84.4 µs per loop (mean ± std. dev. of 10 runs, 100 loops each)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit -o -r 10 load_sample_batched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b022a9ee-82a5-47b3-9904-5413705fa95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "transform = tonic.transforms.Compose([torch.from_numpy,\n",
    "                                      torchvision.transforms.RandomRotation([-10,10])])\n",
    "\n",
    "cached_trainset = DiskCachedDataset(trainset, transform=transform, cache_path='./cache/nmnist/train')\n",
    "\n",
    "# no augmentations for the testset\n",
    "cached_testset = DiskCachedDataset(testset, cache_path='./cache/nmnist/test')\n",
    "\n",
    "batch_size = 128\n",
    "trainloader = DataLoader(cached_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
    "testloader = DataLoader(cached_testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaa39cf6-0361-49fb-afa8-7b6100fa2812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import utils\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "167422d5-e399-441b-8c59-8a082e10ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# neuron and simulation parameters\n",
    "spike_grad = surrogate.atan()\n",
    "beta = 0.5\n",
    "\n",
    "#  Initialize Network\n",
    "net = nn.Sequential(nn.Conv2d(2, 12, 5),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
    "                    nn.Conv2d(12, 32, 5),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(32*5*5, 10),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
    "                    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cf24038-4d14-4a45-8a6f-32ea4537b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time, we won't return membrane as we don't need it\n",
    "\n",
    "def forward_pass(net, data):\n",
    "  spk_rec = []\n",
    "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "\n",
    "  for step in range(data.size(0)):  # data.size(0) = number of time steps\n",
    "      spk_out, mem_out = net(data[step])\n",
    "      spk_rec.append(spk_out)\n",
    "\n",
    "  return torch.stack(spk_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d96f46d3-30dc-495c-b8b1-e406608079f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-2, betas=(0.9, 0.999))\n",
    "loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c938c047-8231-44d9-b7a8-3c053c71a7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0 \n",
      "Train Loss: 30.96\n",
      "Accuracy: 6.25%\n",
      "\n",
      "Epoch 0, Iteration 1 \n",
      "Train Loss: 31.00\n",
      "Accuracy: 9.38%\n",
      "\n",
      "Epoch 0, Iteration 2 \n",
      "Train Loss: 31.00\n",
      "Accuracy: 8.59%\n",
      "\n",
      "Epoch 0, Iteration 3 \n",
      "Train Loss: 31.00\n",
      "Accuracy: 5.47%\n",
      "\n",
      "Epoch 0, Iteration 4 \n",
      "Train Loss: 26.52\n",
      "Accuracy: 13.28%\n",
      "\n",
      "Epoch 0, Iteration 5 \n",
      "Train Loss: 12.64\n",
      "Accuracy: 14.06%\n",
      "\n",
      "Epoch 0, Iteration 6 \n",
      "Train Loss: 16.78\n",
      "Accuracy: 12.50%\n",
      "\n",
      "Epoch 0, Iteration 7 \n",
      "Train Loss: 19.12\n",
      "Accuracy: 19.53%\n",
      "\n",
      "Epoch 0, Iteration 8 \n",
      "Train Loss: 17.18\n",
      "Accuracy: 20.31%\n",
      "\n",
      "Epoch 0, Iteration 9 \n",
      "Train Loss: 13.82\n",
      "Accuracy: 15.62%\n",
      "\n",
      "Epoch 0, Iteration 10 \n",
      "Train Loss: 13.32\n",
      "Accuracy: 14.06%\n",
      "\n",
      "Epoch 0, Iteration 11 \n",
      "Train Loss: 15.43\n",
      "Accuracy: 17.19%\n",
      "\n",
      "Epoch 0, Iteration 12 \n",
      "Train Loss: 12.69\n",
      "Accuracy: 23.44%\n",
      "\n",
      "Epoch 0, Iteration 13 \n",
      "Train Loss: 11.51\n",
      "Accuracy: 28.91%\n",
      "\n",
      "Epoch 0, Iteration 14 \n",
      "Train Loss: 12.35\n",
      "Accuracy: 28.91%\n",
      "\n",
      "Epoch 0, Iteration 15 \n",
      "Train Loss: 12.67\n",
      "Accuracy: 36.72%\n",
      "\n",
      "Epoch 0, Iteration 16 \n",
      "Train Loss: 12.24\n",
      "Accuracy: 40.62%\n",
      "\n",
      "Epoch 0, Iteration 17 \n",
      "Train Loss: 11.99\n",
      "Accuracy: 35.94%\n",
      "\n",
      "Epoch 0, Iteration 18 \n",
      "Train Loss: 11.28\n",
      "Accuracy: 32.03%\n",
      "\n",
      "Epoch 0, Iteration 19 \n",
      "Train Loss: 12.79\n",
      "Accuracy: 32.81%\n",
      "\n",
      "Epoch 0, Iteration 20 \n",
      "Train Loss: 11.25\n",
      "Accuracy: 35.16%\n",
      "\n",
      "Epoch 0, Iteration 21 \n",
      "Train Loss: 12.11\n",
      "Accuracy: 32.81%\n",
      "\n",
      "Epoch 0, Iteration 22 \n",
      "Train Loss: 10.86\n",
      "Accuracy: 37.50%\n",
      "\n",
      "Epoch 0, Iteration 23 \n",
      "Train Loss: 10.82\n",
      "Accuracy: 32.03%\n",
      "\n",
      "Epoch 0, Iteration 24 \n",
      "Train Loss: 10.29\n",
      "Accuracy: 44.53%\n",
      "\n",
      "Epoch 0, Iteration 25 \n",
      "Train Loss: 10.02\n",
      "Accuracy: 42.19%\n",
      "\n",
      "Epoch 0, Iteration 26 \n",
      "Train Loss: 10.30\n",
      "Accuracy: 45.31%\n",
      "\n",
      "Epoch 0, Iteration 27 \n",
      "Train Loss: 9.96\n",
      "Accuracy: 50.00%\n",
      "\n",
      "Epoch 0, Iteration 28 \n",
      "Train Loss: 9.85\n",
      "Accuracy: 56.25%\n",
      "\n",
      "Epoch 0, Iteration 29 \n",
      "Train Loss: 9.94\n",
      "Accuracy: 54.69%\n",
      "\n",
      "Epoch 0, Iteration 30 \n",
      "Train Loss: 9.84\n",
      "Accuracy: 38.28%\n",
      "\n",
      "Epoch 0, Iteration 31 \n",
      "Train Loss: 9.86\n",
      "Accuracy: 32.03%\n",
      "\n",
      "Epoch 0, Iteration 32 \n",
      "Train Loss: 9.05\n",
      "Accuracy: 46.88%\n",
      "\n",
      "Epoch 0, Iteration 33 \n",
      "Train Loss: 9.10\n",
      "Accuracy: 57.03%\n",
      "\n",
      "Epoch 0, Iteration 34 \n",
      "Train Loss: 9.18\n",
      "Accuracy: 57.03%\n",
      "\n",
      "Epoch 0, Iteration 35 \n",
      "Train Loss: 9.45\n",
      "Accuracy: 53.91%\n",
      "\n",
      "Epoch 0, Iteration 36 \n",
      "Train Loss: 8.62\n",
      "Accuracy: 56.25%\n",
      "\n",
      "Epoch 0, Iteration 37 \n",
      "Train Loss: 8.68\n",
      "Accuracy: 60.16%\n",
      "\n",
      "Epoch 0, Iteration 38 \n",
      "Train Loss: 8.95\n",
      "Accuracy: 53.91%\n",
      "\n",
      "Epoch 0, Iteration 39 \n",
      "Train Loss: 7.78\n",
      "Accuracy: 69.53%\n",
      "\n",
      "Epoch 0, Iteration 40 \n",
      "Train Loss: 8.20\n",
      "Accuracy: 67.19%\n",
      "\n",
      "Epoch 0, Iteration 41 \n",
      "Train Loss: 8.14\n",
      "Accuracy: 50.78%\n",
      "\n",
      "Epoch 0, Iteration 42 \n",
      "Train Loss: 8.11\n",
      "Accuracy: 58.59%\n",
      "\n",
      "Epoch 0, Iteration 43 \n",
      "Train Loss: 7.53\n",
      "Accuracy: 66.41%\n",
      "\n",
      "Epoch 0, Iteration 44 \n",
      "Train Loss: 6.85\n",
      "Accuracy: 75.00%\n",
      "\n",
      "Epoch 0, Iteration 45 \n",
      "Train Loss: 7.15\n",
      "Accuracy: 74.22%\n",
      "\n",
      "Epoch 0, Iteration 46 \n",
      "Train Loss: 7.52\n",
      "Accuracy: 65.62%\n",
      "\n",
      "Epoch 0, Iteration 47 \n",
      "Train Loss: 6.83\n",
      "Accuracy: 73.44%\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 14\u001b[0m spk_rec \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m loss_fn(spk_rec, targets)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Gradient calculation + weight update\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(net, data)\u001b[0m\n\u001b[1;32m      5\u001b[0m utils\u001b[38;5;241m.\u001b[39mreset(net)  \u001b[38;5;66;03m# resets hidden states for all LIF neurons in net\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):  \u001b[38;5;66;03m# data.size(0) = number of time steps\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     spk_out, mem_out \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     spk_rec\u001b[38;5;241m.\u001b[39mappend(spk_out)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(spk_rec)\n",
      "File \u001b[0;32m~/miniforge3/envs/snn-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/snn-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/snn-gpu/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/snn-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/snn-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/snn-gpu/lib/python3.10/site-packages/snntorch/_neurons/leaky.py:193\u001b[0m, in \u001b[0;36mLeaky.forward\u001b[0;34m(self, input_, mem)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_hidden:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_leaky_forward_cases(mem)\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmem_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_state_function_hidden(input_)\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_quant:\n",
      "File \u001b[0;32m~/miniforge3/envs/snn-gpu/lib/python3.10/site-packages/snntorch/_neurons/neurons.py:107\u001b[0m, in \u001b[0;36mSpikingNeuron.mem_reset\u001b[0;34m(self, mem)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generates detached reset signal if mem > threshold.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03mReturns reset.\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m mem_shift \u001b[38;5;241m=\u001b[39m mem \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold\n\u001b[0;32m--> 107\u001b[0m reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspike_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmem_shift\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reset\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "num_iters = 50\n",
    "\n",
    "loss_hist = []\n",
    "acc_hist = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, targets) in enumerate(iter(trainloader)):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        net.train()\n",
    "        spk_rec = forward_pass(net, data)\n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
    "\n",
    "        acc = SF.accuracy_rate(spk_rec, targets)\n",
    "        acc_hist.append(acc)\n",
    "        print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
    "\n",
    "        # training loop breaks after 50 iterations\n",
    "        if i == num_iters:\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0721cc-2702-4f5f-88d9-d6307486315d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db744d1a-044f-47d9-aa84-b596565a59af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
