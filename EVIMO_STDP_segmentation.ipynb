{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ae57a8f-906b-42df-a3cd-87ea4b40129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tonic\n",
    "import tonic.transforms as transforms\n",
    "from torchvision import transforms as tt\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from tonic.dataset import Dataset\n",
    "from typing import Callable, Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from typing import Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "from statistics import mean\n",
    "\n",
    "import snntorch as snn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79b8697-e686-491d-803b-38d72e7e15e5",
   "metadata": {},
   "source": [
    "# ##STDP rule for learning on device\n",
    "local learning\n",
    "focus on updatability, how can we update the model on small hardware\n",
    "Add a new class to \n",
    "Try low class count - is object in path of robot? Does path need to change\n",
    "\n",
    "Make algo to make memory efficient updates on model after already learning through backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c845eef-bb73-4750-a2cb-e4e9d0a54edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ed081d-6cba-45ed-ad66-e041289f9dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./mnist_sg_cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba6837a-0099-4f46-a2c5-939b040a487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import snn_utils\n",
    "import base_model\n",
    "import lenet_decolle_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9168875f-e342-4b9a-93bb-10eb575b42ac",
   "metadata": {},
   "source": [
    "### Notes\n",
    " - 2/14/2024\n",
    "   - May be extremely memory intensive for image segmentation with local learning rule. Would have to return entire mask at each readout layer, 30000 params for even a small mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "921dea1c-258e-4b4a-9573-a662cd127526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cache_dir = \"./data/EVIMO/obj_det_cache\"\n",
    "sensor_size = [640, 480, 2]\n",
    "batch_size = 4\n",
    "num_bins_per_frame = 8 # T = 100\n",
    "framerate = 200\n",
    "epochs=0\n",
    "output_size=(30, 40)\n",
    "input_size=(480, 640)\n",
    "num_classes = 25\n",
    "\n",
    "dtype=torch.float\n",
    "\n",
    "#device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e22b1e91-e46e-49ad-978b-02166931a9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVIMOMask(Dataset):\n",
    "    def __init__(self,\n",
    "                 dir: str,\n",
    "                 num_bins_per_frame: int,\n",
    "                 output_size: Tuple,\n",
    "                ):\n",
    "        self.dir = dir\n",
    "        self.num_bins_per_frame = num_bins_per_frame\n",
    "        self.output_size = output_size\n",
    "        self.length = np.load(self.dir + \"/length.npy\")\n",
    "\n",
    "        \n",
    "        #print(self.length)\n",
    "        #self.start_idx = start_idx\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = np.load(self.dir + \"/\" + str(index) + \".npy\", allow_pickle=True).tolist()\n",
    "\n",
    "\n",
    "        events = np.asarray(item[\"events\"])\n",
    "\n",
    "        frame_transform = transforms.Compose([# transforms.Denoise(filter_time=0.01),\n",
    "                                       transforms.ToVoxelGrid(sensor_size=sensor_size,\n",
    "                                                          n_time_bins=self.num_bins_per_frame)\n",
    "                                      ])\n",
    "\n",
    "        events = frame_transform(events)\n",
    "\n",
    "        mask = torch.from_numpy(np.asarray([item[\"mask\"]])).to(torch.int64)\n",
    "\n",
    "        one_hot_mask = torch.nn.functional.one_hot(mask, num_classes=num_classes).transpose(1, 3).transpose(2, 3) # Conversion into Batch, Channels, H, W\n",
    "\n",
    "        # Downsize the mask.\n",
    "        resized_mask = tt.functional.resize(one_hot_mask, self.output_size, antialias=True)\n",
    "        \n",
    "        return torch.from_numpy(events).to(torch.float), resized_mask.squeeze()\n",
    "\n",
    "    def get_original_mask(self, index):\n",
    "        item = np.load(self.dir + \"/\" + str(index) + \".npy\", allow_pickle=True).tolist()\n",
    "        mask = torch.from_numpy(np.asarray([item[\"mask\"]])).to(torch.int64)\n",
    "        return mask\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length # - self.start_idx\n",
    "\n",
    "    def get_item(self, index):\n",
    "        item = np.load(self.dir + \"/\" + str(index) + \".npy\", allow_pickle=True).tolist()\n",
    "        return item\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ada65f23-35f7-432f-8ce7-a2b5ac4973d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EVIMOMask(dir=\"./data/EVIMO/left_cam/scene13_test5\", output_size=output_size, num_bins_per_frame=num_bins_per_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29ce451a-7367-42d2-9f95-b00262b1d8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "e, mask = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d319f98-2529-4da8-ae4c-e7c782d72a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([25, 30, 40]), tensor([0, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape, mask.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62ac7809-b415-41c7-8b74-a2b7b026e323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([25, 30, 40]), 30000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_output_elements = torch.numel(mask)\n",
    "mask.shape, num_output_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85d4284e-29af-480b-9733-9829a638becb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 480, 640])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a722ca3a-9151-481f-a617-bb31f151cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9d6590c-25e2-49fe-bad6-aa756fe22db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 480, 640])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events, resized_mask = next(iter(trainloader))\n",
    "events = events.to(device)\n",
    "events.shape\n",
    "events[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de9911c7-9034-4e94-b436-4473ae0e4626",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.95\n",
    "\n",
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(input_size[0] * input_size[1], 128)\n",
    "        #self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=61)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(128, output_size[0] * output_size[1] * num_classes)\n",
    "        #self.conv2 = nn.Conv2d(in_channels=16, out_channels=num_classes, kernel_size=5)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Input data should be in format: Batch size, num_bins_per_frame, channel(1), x, y\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        # Record intermediate layer to find spikes in case of new labels\n",
    "        spk1_rec = []\n",
    "\n",
    "        current_batch_size = data.shape[0]\n",
    "        \n",
    "        data = data.transpose(0, 1) # Converting to num_bins_per_frame, batch_size, channel, x, y\n",
    "        \n",
    "\n",
    "        for step in range(num_bins_per_frame):\n",
    "            x = data[step].view(current_batch_size, -1)\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            #print(spk1.shape)\n",
    "            spk1_rec.append(spk1)\n",
    "            \n",
    "            spk2_rec.append(spk2.reshape(current_batch_size, num_classes, output_size[0], output_size[1]))\n",
    "            mem2_rec.append(mem2.reshape(current_batch_size, num_classes, output_size[0], output_size[1]))\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0), torch.stack(spk1_rec, dim=0)\n",
    "\n",
    "# Load the network onto CUDA if available\n",
    "net = Net().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7142d3ea-2e9d-4ebc-a145-f8d73af40079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 25, 30, 40])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spk2, mem2, spk1 = net(events)\n",
    "mem2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e2317c4-3fff-4371-bd4a-036081f5611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_masks(dataset, index, time_step):\n",
    "    data, mask = dataset[index]\n",
    "    data = data.to(device).to(torch.float)\n",
    "    data = data.reshape(1, 8, 1, 480, 640)\n",
    "    \n",
    "    spk_rec, mem_rec, _ = net(data)\n",
    "\n",
    "    output_mask = spk_rec[time_step].squeeze().argmax(dim=0)\n",
    "    mask = mask.argmax(dim=0)\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, nrows=1)\n",
    "    axes.ravel()[0].imshow(output_mask.tolist())\n",
    "    axes.ravel()[0].set_title(\"Model Generated\")\n",
    "    axes.ravel()[0].set_axis_off()\n",
    "    axes.ravel()[1].imshow(mask.tolist())\n",
    "    axes.ravel()[1].set_title(\"Original\")\n",
    "    axes.ravel()[1].set_axis_off()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return output_mask, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6caa261a-721b-4d54-8a13-e6526624d9cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=7e-2, betas=(0.9, 0.999))\n",
    "\n",
    "# Outer training loop\n",
    "net.train()\n",
    "for epoch in range(epochs):\n",
    "    iter_counter = 0\n",
    "    \n",
    "    #train_batch = iter(trainloader)\n",
    "    train_batch = tqdm(iter(trainloader), desc=f\"Epoch {epoch}\")\n",
    "\n",
    "    # Minibatch training loop\n",
    "    acc_each_batch = []\n",
    "    for data, masks in train_batch:\n",
    "        data = data.to(device).to(torch.float)\n",
    "        masks = masks.to(device).to(torch.float)\n",
    "\n",
    "        #print(data.shape)\n",
    "        #current_batch_size = data.shape[0]\n",
    "        #print(data.view(batch_size, -1).shape)\n",
    "        #print(masks.shape)\n",
    "\n",
    "        # forward pass\n",
    "        \n",
    "        spk2_rec, mem2_rec, _ = net(data)\n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "        acc_each_step = []\n",
    "        for step in range(num_bins_per_frame):\n",
    "            loss_val += loss(spk2_rec[step], masks)\n",
    "\n",
    "        \n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "    #print(f\"Mean Pixel-Based Accuracy for all Steps & Batches: {mean(acc_each_batch)}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        compare_masks(dataset, index=0, time_step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04a6e935-10a7-42a6-b72d-4fbcf7635287",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, \"evimo_img_segmentation_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7eb2b93e-4f1c-44e1-91ad-83b6c35c9a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25*30*40*batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3bb3cf1-53e2-4065-9233-7be584e72f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
       "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
       "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [ 0,  0, 12,  ...,  0,  0,  0],\n",
       "         [ 0,  0,  8,  ...,  0,  0,  0],\n",
       "         [ 0,  0,  0,  ...,  0,  0,  0]]),\n",
       " tensor([[ 9,  9,  9,  ...,  0,  0,  0],\n",
       "         [ 0,  9,  9,  ...,  0,  0,  0],\n",
       "         [ 0,  9,  9,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [22, 22, 22,  ..., 22, 22, 22],\n",
       "         [22, 22, 22,  ..., 22, 22, 22],\n",
       "         [22, 22, 22,  ..., 22, 22, 22]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_masks(dataset, index=0, time_step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49486bbf-be5f-4d51-8431-f994406939dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['fc1.weight', 'fc1.bias', 'lif1.threshold', 'lif1.graded_spikes_factor', 'lif1.reset_mechanism_val', 'lif1.beta', 'fc2.weight', 'fc2.bias', 'lif2.threshold', 'lif2.graded_spikes_factor', 'lif2.reset_mechanism_val', 'lif2.beta'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "752cafc7-6873-4d66-b82e-7286ff3eb86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0018)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()[\"fc1.weight\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47be502a-0f46-4e47-9392-9692c86f20a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0018)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()[\"fc1.weight\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9093cf2f-c566-426d-973c-0646fc5a0a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0018)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()[\"fc1.bias\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d06ae2a1-55cd-4956-b29e-be6acae94190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0018)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()[\"fc1.bias\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82966152-fd7d-4ef9-b178-104327b7ca77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2114ff76-e763-4040-a9fe-fd098f53d6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 307200])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e, mask = dataset[0]\n",
    "e.shape\n",
    "\n",
    "e[0].view(1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500f50a-24bb-468f-8abf-49d57717da90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2910bc08-fbfc-4c01-bbb8-0629d71b3074",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load(\"evimo_img_segmentation_model\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad2f16ff-2903-4f17-a4df-17ef54ec22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mem1 = net.lif1.init_leaky()\n",
    "# mem2 = net.lif2.init_leaky()\n",
    "# weights1 = net.state_dict()[\"fc1.weight\"]\n",
    "# weights2 = net.state_dict()[\"fc2.weight\"]\n",
    "\n",
    "# spk1_rec = [] # A list of lists of spike indexes.\n",
    "# spk2_rec = []\n",
    "\n",
    "# for step in range(num_bins_per_frame):\n",
    "#     x = e[step].view(1, -1).to(device) # Batch size 1.\n",
    "\n",
    "#     cur1 = net.fc1(x)\n",
    "#     spk1, mem1 = net.lif1(cur1, mem1)\n",
    "#     spk1_idx = spk1.nonzero(as_tuple=True)[1]\n",
    "#     spk1_rec.append(spk1_idx)\n",
    "\n",
    "#     cur2 = net.fc2(spk1)\n",
    "#     spk2, mem2 = net.lif2(cur2, mem2)\n",
    "\n",
    "#     spk2_idx = spk2.nonzero(as_tuple=True)[1]\n",
    "#     spk2_rec.append(spk2_idx)\n",
    "\n",
    "# do_stdp(weights2, spk1_rec, spk2_rec)\n",
    "\n",
    "    \n",
    "# \"\"\"\n",
    "# Initially train surrogate grad/backprop\n",
    "\n",
    "# Learn after with stdp, so that we don't need specific labels\n",
    "\n",
    "# How would we account for new labels, would require change in output space\n",
    "#     More general labels\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d45d5b8a-94fc-4bf0-9692-34714c9bba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = net.state_dict()[\"fc1.weight\"]\n",
    "weights2 = net.state_dict()[\"fc2.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53759c4-beb7-4780-8281-1c418248b148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab38dee6-2fbe-49a9-ad7f-cf40cf8539a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_stdp(weights, spk1_idxs, spk2_idxs):\n",
    "    # BEFORE SPIKE WEIGHT INCREASE\n",
    "    for main_step in range(0, num_bins_per_frame):\n",
    "        current_spk2 = spk2_idxs[main_step]\n",
    "        for i in range(0, main_step): # For each time step that passed:\n",
    "            for input_spike_index in spk1_idxs[i]:\n",
    "                # spk2_idx = list of indices\n",
    "                # input_spike_index = single index\n",
    "                # The locations defined by all spk2_idx and all spk1_idx are the weights that should be increased\n",
    "                weights[current_spk2, input_spike_index] += 0.05*i\n",
    "\n",
    "def post_stdp(weights, spk1_idxs, spk2_idxs):\n",
    "    # AFTER SPIKE WEIGHT DECREASE\n",
    "    for main_step in range(0, num_bins_per_frame):\n",
    "        for current_spk2 in spk2_idxs[main_step]: # Retrieve only a single index of a spike in layer 2\n",
    "    \n",
    "            for secondary_step in range(main_step+1, num_bins_per_frame): \n",
    "                current_spk1s = spk1_idxs[secondary_step] # Retrieve all indices of spikes in layer 1 for the time step\n",
    "        \n",
    "                for input_spike_index in current_spk1s:\n",
    "                    weights[current_spk2, input_spike_index] -= 0.05 * (main_step/secondary_step)\n",
    "\n",
    "\n",
    "def do_stdp(weights, spk1_idxs, spk2_idxs):\n",
    "    pre_stdp(weights, spk1_idxs, spk2_idxs)\n",
    "    post_stdp(weights, spk1_idxs, spk2_idxs)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "926b51dc-fb22-444c-91a2-d9ccab347896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cl_forward(data):\n",
    "    # Input data should be in format: Batch size, num_bins_per_frame, channel(1), x, y\n",
    "\n",
    "    # Initialize hidden states at t=0\n",
    "    mem1 = net.lif1.init_leaky()\n",
    "    mem2 = net.lif2.init_leaky()\n",
    "\n",
    "    spk1_rec = []\n",
    "    spk1_idxs = []\n",
    "    \n",
    "    # Record the final layer\n",
    "    spk2_rec = []\n",
    "    spk2_idxs = []\n",
    "    mem2_rec = []\n",
    "\n",
    "    current_batch_size = 1 #data.shape[0]\n",
    "    \n",
    "    #data = data.transpose(0, 1) # Converting to num_bins_per_frame, batch_size, channel, x, y\n",
    "\n",
    "    for step in range(num_bins_per_frame):\n",
    "        x = data[step].view(current_batch_size, -1)\n",
    "        \n",
    "        cur1 = net.fc1(x)\n",
    "        spk1, mem1 = net.lif1(cur1, mem1)\n",
    "        spk1_idx = spk1.nonzero(as_tuple=True)[1]\n",
    "        spk1_idxs.append(spk1_idx)\n",
    "    \n",
    "        cur2 = net.fc2(spk1)\n",
    "        spk2, mem2 = net.lif2(cur2, mem2)\n",
    "        \n",
    "        spk2_rec.append(spk2)\n",
    "        mem2_rec.append(mem2)\n",
    "\n",
    "        #print(spk2.shape)\n",
    "    \n",
    "        spk2_idx = spk2.nonzero(as_tuple=True)[1]\n",
    "        #print(spk2_idx[0].unique())\n",
    "        #print(spk2_idx.reshape(2, spk2_idx.shape[0])[0].unique())\n",
    "        spk2_idxs.append(spk2_idx)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    do_stdp(weights2, spk1_idxs, spk2_idxs)\n",
    "\n",
    "    return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4abf0d0-cff9-4b1c-b9e7-8d8ea79f46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c20a5d3f-bb24-4968-9266-05a031845471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 1, 480, 640])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b7ef0f1-4f70-47c9-807a-ad472c0d5cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 480, 640])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46e22aab-660d-44e7-8dac-4b4c8e5082ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcl_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 44\u001b[0m, in \u001b[0;36mcl_forward\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m#print(spk2_idx[0].unique())\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#print(spk2_idx.reshape(2, spk2_idx.shape[0])[0].unique())\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     spk2_idxs\u001b[38;5;241m.\u001b[39mappend(spk2_idx)\n\u001b[0;32m---> 44\u001b[0m \u001b[43mdo_stdp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspk1_idxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspk2_idxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(spk2_rec, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), torch\u001b[38;5;241m.\u001b[39mstack(mem2_rec, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[30], line 26\u001b[0m, in \u001b[0;36mdo_stdp\u001b[0;34m(weights, spk1_idxs, spk2_idxs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_stdp\u001b[39m(weights, spk1_idxs, spk2_idxs):\n\u001b[1;32m     25\u001b[0m     pre_stdp(weights, spk1_idxs, spk2_idxs)\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mpost_stdp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspk1_idxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspk2_idxs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 21\u001b[0m, in \u001b[0;36mpost_stdp\u001b[0;34m(weights, spk1_idxs, spk2_idxs)\u001b[0m\n\u001b[1;32m     18\u001b[0m current_spk1s \u001b[38;5;241m=\u001b[39m spk1_idxs[secondary_step] \u001b[38;5;66;03m# Retrieve all indices of spikes in layer 1 for the time step\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_spike_index \u001b[38;5;129;01min\u001b[39;00m current_spk1s:\n\u001b[0;32m---> 21\u001b[0m     weights[current_spk2, input_spike_index] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m \u001b[38;5;241m*\u001b[39m (main_step\u001b[38;5;241m/\u001b[39msecondary_step)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cl_forward(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2baaaa7-f559-4971-a709-bfcb2e1781cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e992ebe-2d4a-43c6-ae03-aa1dcb95a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_finder(spk_rec):\n",
    "    pixels = []\n",
    "    t = torch.where(spk_rec == 0, True, False)\n",
    "    for row in range(t.shape[0]):\n",
    "        for col in range(t.shape[1]):\n",
    "            if t[row][col].all(): # All 25 labels are 0\n",
    "                pixels.append((row, col))\n",
    "    return pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c1d1b77-9e2b-41e8-a378-407c76d9f7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 480, 640])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f0bb3bb-2d2f-4162-8084-446822275497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 4.7685e-05, 9.5370e-05,  ..., 3.3286e+02, 3.4930e+02,\n",
       "        4.1934e+02])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "744af329-53e5-4a8a-b1e3-f2d8cbe26a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 1, 480, 640])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.zeros(4, 8, 1, 480, 640)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a981b92f-620a-442b-a03d-aff1a0bcbd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 30, 40])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    spk1_idxs = []\n",
    "    spk2_idxs = []\n",
    "    data = test.to(device).to(torch.float)\n",
    "    masks = y.to(device).to(torch.float)\n",
    "\n",
    "    spk2_rec, mem2_rec, spk1_rec = net(data)\n",
    "    \n",
    "    for spk1 in spk1_rec:\n",
    "        spk1_idx = spk1.nonzero(as_tuple=True)[1]\n",
    "        spk1_idxs.append(spk1_idx)\n",
    "\n",
    "    for spk2 in spk2_rec:\n",
    "        spk2_idx = spk2.nonzero(as_tuple=True)[1]\n",
    "        spk2_idxs.append(spk2_idx)\n",
    "\n",
    "spk2_rec = spk2_rec[-1][0]\n",
    "spk2_rec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebd00783-51ef-47a5-a48e-c8382a2546a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = pixel_finder(spk2_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d479df5-aa52-4ab6-b756-d6ec66d92c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_mask = spk2_rec.squeeze().argmax(dim=0)\n",
    "\n",
    "for row, col in pixels:\n",
    "    output_mask[row][col] = 2\n",
    "\n",
    "fig, axes = plt.subplots(ncols=1, nrows=1)\n",
    "axes.imshow(output_mask.tolist())\n",
    "#axes.set_title(\"Model Generated\")\n",
    "axes.set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3791f851-5417-4bff-b469-24b6452c9265",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ae43313e-006e-411a-ac73-053d42361d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30000, 128])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e29a86-1122-40d2-b8ca-613a2cc16ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "25*30*40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1732616c-74af-45a7-bd3f-2703b0943953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 30, 40, 128])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = weights2.reshape((25, 30, 40, 128))\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de2b8b85-7275-4c4e-be8a-defebd0d9f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 307200])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a567746e-3d81-4d98-b914-23be99db6968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 128])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spk1_rec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f400c9a4-d72a-4683-858c-d83f93f17c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
