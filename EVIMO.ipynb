{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c61e32fa-4662-4c2f-8820-130bf9fa13be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tonic\n",
    "import tonic.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from tonic.dataset import Dataset\n",
    "from typing import Callable, Optional\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import matplotlib.patches as patches\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b30097a0-45a9-41a5-92c4-18344bedafed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./mnist_sg_cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2b519d7-7625-47ad-bf9e-4bfea280d6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import snn_utils\n",
    "import base_model\n",
    "import lenet_decolle_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb4260d3-1022-4b5c-816d-d15b6322f461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.cuda.device at 0x7fb550a46e60>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[torch.cuda.device(i) for i in range(torch.cuda.device_count())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2440213f-255c-496c-8cc3-e3167d74f316",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"/media/user/EVIMO/raw/imo/eval/scene15_dyn_test_01/left_camera/ground_truth_000000\"\n",
    "sensor_size = [640, 480, 2]\n",
    "batch_size = 4\n",
    "framerate = 200\n",
    "device = 'cuda'\n",
    "epochs=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6915dd24-0ae7-460e-a572-88ccd59905a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EVIMO(Dataset):\n",
    "#     def __init__(self,\n",
    "#                  data: np.array,\n",
    "#                  masks: list,\n",
    "#                  transform: Optional[Callable] = None\n",
    "#                 ):\n",
    "#         #super().__init__(save_to, transform=transform)\n",
    "#         self.masks = masks\n",
    "#         if transform is not None:\n",
    "#             self.data = transform(data)\n",
    "#             if len(self.data) != len(self.masks):\n",
    "#                 raise ValueError(\"Number of frames after transforms does not match number of masks\")\n",
    "#         else:\n",
    "#             self.data = data\n",
    "\n",
    "#         #target = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "#         self.targets = []\n",
    "        \n",
    "#         num_masks_include = 0\n",
    "#         for mask in self.masks:\n",
    "#             if 23 in mask:\n",
    "#                 num_masks_include += 1\n",
    "#                 self.targets.append([0, 1])\n",
    "#             else:\n",
    "#                 self.targets.append([1, 0])\n",
    "\n",
    "#         print(f\"Target present in: {num_masks_include} of {len(self.masks)} masks. {num_masks_include/len(self.masks)}\")\n",
    "            \n",
    "\n",
    "#         self.targets = np.asarray(self.targets)\n",
    "        \n",
    "#         #self.targets = torch.from_numpy(np.array(len(self.data)*[target]))\n",
    "        \n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         #return self.data[index], self.targets[index]\n",
    "#         return {\"data\" : self.data[index], \"mask\": self.masks[index], \"target\": self.targets[index]}\n",
    "#     def __len__(self) -> int: \n",
    "#         return len(self.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eee4657b-59c4-4dd1-8f51-d9492c27f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVIMO(Dataset):\n",
    "    def __init__(self,\n",
    "                 dir: str,\n",
    "                 start_idx: int,\n",
    "                 length: int,\n",
    "                ):\n",
    "        self.dir = dir\n",
    "        self.length = length\n",
    "        self.start_idx = start_idx\n",
    "\n",
    "        self.labels = np.load(self.dir + \"/labels.npy\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\"data\": np.load(self.dir + \"/\" + str(index + self.start_idx) + \".npy\"), \"target\": self.labels[index + self.start_idx]}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length - self.start_idx\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3cb1ea2-24d7-4457-b743-1678b5ab5fc1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Denoise removes isolated, one-off events\n",
    "# # time_window\n",
    "# frame_transform = transforms.Compose([#transforms.Denoise(filter_time=10000),\n",
    "#                                       transforms.ToFrame(sensor_size=sensor_size,\n",
    "#                                                          time_window=5000)\n",
    "#                                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d84cf6b1-6549-48cb-adbb-7511b28d4bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tonic_dataset = EVIMO(dir=\"./data/EVIMO/train\", start_idx=285, length=703)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c71e67fd-9b03-4990-979d-cf1aa944bf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7027027027027026"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-0.2972972972972973"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26322418-e8b3-4f14-ad34-efd15c36fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frame(frame, mask): # TODO: Put the mask processing into bounding box into the tonic dataset?\n",
    "    print(mask)\n",
    "    img = frame[1] - frame[0]\n",
    "    fig, ax = plt.subplots(1,2, figsize=(10,10))\n",
    "    ax[0].imshow(img)\n",
    "\n",
    "    mask = torch.from_numpy(mask)\n",
    "    obj_ids = torch.unique(mask)\n",
    "    #print(obj_ids)\n",
    "    obj_ids = obj_ids[1:]\n",
    "    print(obj_ids)\n",
    "    detection_masks = mask == obj_ids[:, None, None]\n",
    "    #print(detection_masks)\n",
    "    \n",
    "    boxes = masks_to_boxes(detection_masks)\n",
    "    print(boxes)\n",
    "    #for box in boxes:\n",
    "    box = boxes[5]\n",
    "    ax[0].add_patch(patches.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], linewidth=1, edgecolor='r', facecolor='none'))\n",
    "\n",
    "\n",
    "    ax[1].imshow(detection_masks[5])\n",
    "    print(f\"obj 23 in mask: {23 in mask}\")\n",
    "    \n",
    "    ax[0].axis(\"off\")\n",
    "    ax[1].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#item = tonic_dataset.__getitem__(600)\n",
    "\n",
    "#print(binned_events.shape)\n",
    "#plot_frame(item['data'], item['mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7fa9a80-586b-4ace-b999-c9d0cd987723",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_trainloader = DataLoader(tonic_dataset, batch_size=batch_size, shuffle=True) # collate_fn=tonic.collation.PadTensors(),\n",
    "single_trainloader = DataLoader(tonic_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7fa7d1e-a4e4-461a-90c3-4b663b07f1bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for i_batch, sample_batched in enumerate(trainloader):\n",
    "#    print(i_batch, sample_batched['data'].size(), sample_batched['target'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f582f111-a1af-4ee6-adeb-b4bd1f13f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datait = iter(batched_trainloader)\n",
    "#batched_data = next(datait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45aaaff3-d489-42d5-9ecb-0489e2a53c0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#batched_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36bcc119-507e-4816-8847-991642b22308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batched_data['target'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "321a0824-3a31-4af3-b745-e4c5f7d5eee1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER SIZE: 2439712\n",
      "STDV: 0.00032011109219189804\n",
      "LAYER SIZE: 298304\n",
      "STDV: 0.0009154623036203756\n",
      "LAYER SIZE: 34048\n",
      "STDV: 0.002709718654762875\n",
      "LAYER SIZE: 512\n",
      "STDV: 0.022097086912079608\n",
      "LAYER SIZE: 32\n",
      "STDV: 0.08838834764831843\n",
      "LAYER SIZE: 64\n",
      "STDV: 0.0625\n",
      "LAYER SIZE: 128\n",
      "STDV: 0.044194173824159216\n"
     ]
    }
   ],
   "source": [
    "item = next(iter(batched_trainloader))\n",
    "data = item['data']\n",
    "#target = item['target']\n",
    "\n",
    "def decolle_loss(r, s, tgt):\n",
    "    loss_tv = 0\n",
    "    for i in range(len(r)):\n",
    "        #print(f\"Loss Readout shape : {r[i].shape}\")\n",
    "        #print(f\"Loss Target shape : {tgt.shape}\")\n",
    "        loss_tv += loss(r[i],tgt) \n",
    "    return loss_tv\n",
    "\n",
    "loss = torch.nn.SmoothL1Loss()\n",
    "\n",
    "convnet_sg = lenet_decolle_model.LenetDECOLLE(out_channels=2,\n",
    "                    Nhid=[32, 64, 128, 256], #Number of convolution channels\n",
    "                    Mhid=[32, 64, 128],\n",
    "                    kernel_size=[4, 8, 16, 32],\n",
    "                    pool_size=[2, 4, 4, 8],\n",
    "                    input_shape=[2, 480, 640],  # data.shape[1:],\n",
    "                    alpha=[.95],\n",
    "                    alpharp=[.65],\n",
    "                    beta=[.92],\n",
    "                    num_conv_layers=4,\n",
    "                    num_mlp_layers=3,\n",
    "                    lc_ampl=0.5).to(device)\n",
    "\n",
    "#net = lenet_decolle_model.LenetDECOLLE(Nhid=[1,8],Mhid=[32,64],out_channels=2, input_shape=[2, 480, 640]).to(device)\n",
    "\n",
    "#convnet_sg\n",
    "\n",
    "data_d = data.to(device)\n",
    "#target_d = target.to(device)\n",
    "convnet_sg.init_parameters_evimo(data_d) # Modifies readout dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e26c9fd9-1b04-4d7d-b4fc-a059bac83ce8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0916f05f-2752-4d2d-bdcb-427f9ac049b9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data_d.transpose(0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f839e066-2097-48bc-ab39-baa0e62f7eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "553f03ca-b3ef-4c1c-9662-52a31ac76679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7408e907-b4a4-4eca-8a5d-e3943bd68333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88e62e65-490a-4c02-b2ac-f5070f4280fe",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc9b051810148a98557241696d8e7a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error tensor(0.5457)\n",
      "Training accuracy tensor(0.4543)\n",
      "Epoch 0 Loss tensor(1.4462, device='cuda:0')\n",
      "Training Error tensor(0.4928)\n",
      "Training accuracy tensor(0.5072)\n",
      "Epoch 1 Loss tensor(1.2820, device='cuda:0')\n",
      "Training Error tensor(0.4928)\n",
      "Training accuracy tensor(0.5072)\n",
      "Epoch 2 Loss tensor(1.4299, device='cuda:0')\n",
      "Training Error tensor(0.4904)\n",
      "Training accuracy tensor(0.5096)\n",
      "Epoch 3 Loss tensor(1.3389, device='cuda:0')\n",
      "Training Error tensor(0.5168)\n",
      "Training accuracy tensor(0.4832)\n",
      "Epoch 4 Loss tensor(1.3703, device='cuda:0')\n",
      "Training Error tensor(0.4808)\n",
      "Training accuracy tensor(0.5192)\n",
      "Epoch 5 Loss tensor(1.3552, device='cuda:0')\n",
      "Training Error tensor(0.4928)\n",
      "Training accuracy tensor(0.5072)\n",
      "Epoch 6 Loss tensor(1.2163, device='cuda:0')\n",
      "Training Error tensor(0.4688)\n",
      "Training accuracy tensor(0.5312)\n",
      "Epoch 7 Loss tensor(1.3063, device='cuda:0')\n",
      "Training Error tensor(0.5264)\n",
      "Training accuracy tensor(0.4736)\n",
      "Epoch 8 Loss tensor(1.3327, device='cuda:0')\n",
      "Training Error tensor(0.5024)\n",
      "Training accuracy tensor(0.4976)\n",
      "Epoch 9 Loss tensor(1.4172, device='cuda:0')\n",
      "Training Error tensor(0.4639)\n",
      "Training accuracy tensor(0.5361)\n",
      "Epoch 10 Loss tensor(1.1625, device='cuda:0')\n",
      "Training Error tensor(0.5337)\n",
      "Training accuracy tensor(0.4663)\n",
      "Epoch 11 Loss tensor(1.2323, device='cuda:0')\n",
      "Training Error tensor(0.5000)\n",
      "Training accuracy tensor(0.5000)\n",
      "Epoch 12 Loss tensor(1.0180, device='cuda:0')\n",
      "Training Error tensor(0.4736)\n",
      "Training accuracy tensor(0.5264)\n",
      "Epoch 13 Loss tensor(1.2752, device='cuda:0')\n",
      "Training Error tensor(0.4880)\n",
      "Training accuracy tensor(0.5120)\n",
      "Epoch 14 Loss tensor(1.2519, device='cuda:0')\n",
      "Training Error tensor(0.5048)\n",
      "Training accuracy tensor(0.4952)\n",
      "Epoch 15 Loss tensor(1.2492, device='cuda:0')\n",
      "Training Error tensor(0.4904)\n",
      "Training accuracy tensor(0.5096)\n",
      "Epoch 16 Loss tensor(1.2743, device='cuda:0')\n",
      "Training Error tensor(0.4832)\n",
      "Training accuracy tensor(0.5168)\n",
      "Epoch 17 Loss tensor(1.3722, device='cuda:0')\n",
      "Training Error tensor(0.5024)\n",
      "Training accuracy tensor(0.4976)\n",
      "Epoch 18 Loss tensor(1.5302, device='cuda:0')\n",
      "Training Error tensor(0.4736)\n",
      "Training accuracy tensor(0.5264)\n",
      "Epoch 19 Loss tensor(1.3336, device='cuda:0')\n",
      "Training Error tensor(0.5144)\n",
      "Training accuracy tensor(0.4856)\n",
      "Epoch 20 Loss tensor(1.4322, device='cuda:0')\n",
      "Training Error tensor(0.5216)\n",
      "Training accuracy tensor(0.4784)\n",
      "Epoch 21 Loss tensor(1.3103, device='cuda:0')\n",
      "Training Error tensor(0.5337)\n",
      "Training accuracy tensor(0.4663)\n",
      "Epoch 22 Loss tensor(1.4340, device='cuda:0')\n",
      "Training Error tensor(0.4808)\n",
      "Training accuracy tensor(0.5192)\n",
      "Epoch 23 Loss tensor(1.2524, device='cuda:0')\n",
      "Training Error tensor(0.5048)\n",
      "Training accuracy tensor(0.4952)\n",
      "Epoch 24 Loss tensor(1.4464, device='cuda:0')\n",
      "Training Error tensor(0.5000)\n",
      "Training accuracy tensor(0.5000)\n",
      "Epoch 25 Loss tensor(1.3900, device='cuda:0')\n",
      "Training Error tensor(0.4712)\n",
      "Training accuracy tensor(0.5288)\n",
      "Epoch 26 Loss tensor(1.2545, device='cuda:0')\n",
      "Training Error tensor(0.5096)\n",
      "Training accuracy tensor(0.4904)\n",
      "Epoch 27 Loss tensor(1.2373, device='cuda:0')\n",
      "Training Error tensor(0.5192)\n",
      "Training accuracy tensor(0.4808)\n",
      "Epoch 28 Loss tensor(1.4740, device='cuda:0')\n",
      "Training Error tensor(0.5096)\n",
      "Training accuracy tensor(0.4904)\n",
      "Epoch 29 Loss tensor(1.2627, device='cuda:0')\n",
      "Training Error tensor(0.4880)\n",
      "Training accuracy tensor(0.5120)\n",
      "Epoch 30 Loss tensor(1.0603, device='cuda:0')\n",
      "Training Error tensor(0.4856)\n",
      "Training accuracy tensor(0.5144)\n",
      "Epoch 31 Loss tensor(1.2577, device='cuda:0')\n",
      "Training Error tensor(0.5144)\n",
      "Training accuracy tensor(0.4856)\n",
      "Epoch 32 Loss tensor(1.2900, device='cuda:0')\n",
      "Training Error tensor(0.4639)\n",
      "Training accuracy tensor(0.5361)\n",
      "Epoch 33 Loss tensor(1.2638, device='cuda:0')\n",
      "Training Error tensor(0.4663)\n",
      "Training accuracy tensor(0.5337)\n",
      "Epoch 34 Loss tensor(1.2155, device='cuda:0')\n",
      "Training Error tensor(0.4880)\n",
      "Training accuracy tensor(0.5120)\n",
      "Epoch 35 Loss tensor(1.2408, device='cuda:0')\n",
      "Training Error tensor(0.4976)\n",
      "Training accuracy tensor(0.5024)\n",
      "Epoch 36 Loss tensor(1.4900, device='cuda:0')\n",
      "Training Error tensor(0.4784)\n",
      "Training accuracy tensor(0.5216)\n",
      "Epoch 37 Loss tensor(1.2509, device='cuda:0')\n",
      "Training Error tensor(0.5409)\n",
      "Training accuracy tensor(0.4591)\n",
      "Epoch 38 Loss tensor(1.4518, device='cuda:0')\n",
      "Training Error tensor(0.5000)\n",
      "Training accuracy tensor(0.5000)\n",
      "Epoch 39 Loss tensor(1.1594, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "opt_conv = torch.optim.Adamax(convnet_sg.get_trainable_parameters(), lr=1e-4, betas=[0., .95])\n",
    "\n",
    "for e in tqdm(range(epochs), desc=\"Epoch\"):        \n",
    "    error = []\n",
    "    accuracy=[]\n",
    "    for item in iter(batched_trainloader):\n",
    "        data = item['data']\n",
    "        label = item['target']\n",
    "\n",
    "        if label.shape[0] != 4:\n",
    "            #print(f\"last entry\")\n",
    "            break\n",
    "        \n",
    "        convnet_sg.train()\n",
    "        loss_hist = 0\n",
    "        data_d = data.to(device)\n",
    "        label_d = label.to(device)\n",
    "        convnet_sg.init_evimo(data_d)\n",
    "        readout = 0\n",
    "        \n",
    "        st, rt, ut = convnet_sg.forward(data_d)        \n",
    "        loss_tv = decolle_loss(rt, st, label_d)\n",
    "        \n",
    "        loss_tv.backward()\n",
    "        opt_conv.step()\n",
    "        opt_conv.zero_grad()\n",
    "        loss_hist += loss_tv\n",
    "        readout += rt[-1]\n",
    "\n",
    "        #print(f\"Readout: {readout}\")\n",
    "        error += (readout.argmax(axis=1)!=label_d.argmax(axis=1)).float()\n",
    "        accuracy+=(readout.argmax(axis=1)==label_d.argmax(axis=1)).float()\n",
    "        \n",
    "    print('Training Error', torch.mean(torch.Tensor(error)).data)\n",
    "    print('Training accuracy', torch.mean(torch.Tensor(accuracy)).data)     \n",
    "    print('Epoch', e, 'Loss', loss_hist.data)\n",
    "    PATH = './EVIMO_class_cnn.pth'  \n",
    "    torch.save(convnet_sg.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9961fd94-fc2f-41db-aab0-25a5539903c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loss_tv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "848ee114-ef61-4a57-9d90-f28f99147570",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_d shape: torch.Size([4, 2, 480, 640])\n",
      "data_d_tranpose shape: torch.Size([2, 4, 480, 640])\n",
      "Len convnet: 7\n",
      "data init shape: torch.Size([2, 480, 640])\n",
      "Test data shape: torch.Size([2, 480, 640])\n",
      "Label shape: torch.Size([4, 2])\n",
      "Readout\n",
      "[tensor([[0.1892, 0.1854],\n",
      "        [0.2004, 0.1661],\n",
      "        [0.0088, 0.0892],\n",
      "        [0.2712, 0.1167]], device='cuda:0'), tensor([[ 0.1112,  0.0547],\n",
      "        [ 0.2483,  0.1166],\n",
      "        [ 0.2613, -0.0366],\n",
      "        [ 0.2711,  0.2221]], device='cuda:0'), tensor([[0.2608, 0.1217],\n",
      "        [0.4984, 0.3228],\n",
      "        [0.4693, 0.3460],\n",
      "        [0.5119, 0.2287]], device='cuda:0'), tensor([[0.8808, 0.3838],\n",
      "        [0.8262, 0.7793],\n",
      "        [0.9797, 0.6277],\n",
      "        [0.6476, 0.5369]], device='cuda:0'), tensor([[-0.2648,  0.4526],\n",
      "        [-0.1730,  0.2042],\n",
      "        [ 0.0883,  0.0401],\n",
      "        [-0.2005,  0.1219]], device='cuda:0'), tensor([[0.4463, 0.0566],\n",
      "        [0.1227, 0.1192],\n",
      "        [0.2626, 0.0690],\n",
      "        [0.2783, 0.5399]], device='cuda:0'), tensor([[0.4841, 0.5412],\n",
      "        [0.2793, 0.2462],\n",
      "        [0.3085, 0.4606],\n",
      "        [0.5333, 0.3154]], device='cuda:0')]\n",
      "torch.Size([4, 2])\n",
      "Readout: tensor([[0.4841, 0.5412],\n",
      "        [0.2793, 0.2462],\n",
      "        [0.3085, 0.4606],\n",
      "        [0.5333, 0.3154]], device='cuda:0')\n",
      "Output: [1 0 1 0]\n",
      "Labels: tensor([[1, 0],\n",
      "        [1, 0],\n",
      "        [0, 1],\n",
      "        [1, 0]], device='cuda:0')\n",
      "Labels: [0 0 1 0]\n",
      "Testing Error tensor(0.2500)\n",
      "Testing accuracy tensor(0.7500)\n"
     ]
    }
   ],
   "source": [
    "error = []\n",
    "accuracy=[]\n",
    "y_pred = []\n",
    "y_true = []\n",
    "#convnet_sg.requires_init = True\n",
    "for item in iter(batched_trainloader):\n",
    "    #print(\"Item\")\n",
    "    #print(item)\n",
    "    data = item['data']\n",
    "    label = item['target']\n",
    "    if label.shape[0] != 4:\n",
    "        #print(f\"last entry: {label}\")\n",
    "        break\n",
    "\n",
    "    #print(\"earliest label\")\n",
    "    #print(label)\n",
    "    #print(label.shape)\n",
    "\n",
    "    \n",
    "    loss_hist = 0\n",
    "    data_d = data.to(device)\n",
    "    label_d = label.to(device)\n",
    "    print(f\"data_d shape: {data_d.shape}\")\n",
    "    print(f\"data_d_tranpose shape: {data_d.transpose(0,1).shape}\")\n",
    "    print(f\"Len convnet: {len(convnet_sg)}\")\n",
    "    print(f\"data init shape: {data_d.transpose(0,1)[:, 0, :, :].shape}\")\n",
    "    print(f\"Test data shape: {data_d[0].shape}\")\n",
    "    \n",
    "    convnet_sg.init_evimo(data_d)\n",
    "    readout = 0\n",
    "    with torch.no_grad():\n",
    "        #for i in range(0, len(data_d)):\n",
    "        #d = data_d[i:i+1]\n",
    "        \n",
    "        #print(d.shape)\n",
    "        \n",
    "        st, rt, ut = convnet_sg.forward(data_d)\n",
    "\n",
    "        \n",
    "        \n",
    "        print(f\"Label shape: {label_d.shape}\")\n",
    "        print(\"Readout\")\n",
    "        print(rt)\n",
    "        print(rt[0].shape)\n",
    "\n",
    "        #print(\"labels\")\n",
    "        #print(label_d)\n",
    "        #print(label_d.shape)\n",
    "        \n",
    "        \n",
    "        loss_tv = decolle_loss(rt, st, label_d)\n",
    "        \n",
    "        \n",
    "        loss_hist += loss_tv\n",
    "        readout += rt[-1]\n",
    "        print(f\"Readout: {readout}\")\n",
    "        output = (readout.argmax(axis=1)).data.cpu().numpy()\n",
    "        print(f\"Output: {output}\")\n",
    "        y_pred.extend(output)\n",
    "        #labels = label_d.argmax(axis=1)\n",
    "        print(f\"Labels: {label_d}\")\n",
    "        labels = (label_d.argmax(axis=1)).data.cpu().numpy()\n",
    "        print(f\"Labels: {labels}\")\n",
    "        y_true.extend(labels)\n",
    "        accuracy+=(readout.argmax(axis=1)==label_d.argmax(axis=1)).float()\n",
    "        error += (readout.argmax(axis=1)!=label_d.argmax(axis=1)).float()\n",
    "        break\n",
    "        \n",
    "print('Testing Error', torch.mean(torch.Tensor(error)).data)\n",
    "print('Testing accuracy', torch.mean(torch.Tensor(accuracy)).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c14e8c82-256e-490b-b93b-7f5ac7d7493d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0]\n",
      "[0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_true)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1b2bc5a-e41a-4848-b296-e23b3c12b4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nconvnet_sg = lenet_decolle_model.LenetDECOLLE( out_channels=2,\\n                    Nhid=[16,32], #Number of convolution channels\\n                    Mhid=[64],\\n                    kernel_size=[7],\\n                    pool_size=[2,2],\\n                    input_shape=[2, 480, 640],  # data.shape[1:],\\n                    alpha=[.95],\\n                    alpharp=[.65],\\n                    beta=[.92],\\n                    num_conv_layers=2,\\n                    num_mlp_layers=1,\\n                    lc_ampl=.5).to(device)\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "convnet_sg = lenet_decolle_model.LenetDECOLLE( out_channels=2,\n",
    "                    Nhid=[16,32], #Number of convolution channels\n",
    "                    Mhid=[64],\n",
    "                    kernel_size=[7],\n",
    "                    pool_size=[2,2],\n",
    "                    input_shape=[2, 480, 640],  # data.shape[1:],\n",
    "                    alpha=[.95],\n",
    "                    alpharp=[.65],\n",
    "                    beta=[.92],\n",
    "                    num_conv_layers=2,\n",
    "                    num_mlp_layers=1,\n",
    "                    lc_ampl=.5).to(device)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "685f938a-2daa-4af3-98e2-ddf8be8f91b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER SIZE: 1228800\n",
      "STDV: 0.00045105489780439506\n",
      "LAYER SIZE: 614400\n",
      "STDV: 0.000637887953849786\n",
      "LAYER SIZE: 64\n",
      "STDV: 0.0625\n",
      "torch.Size([1, 2, 480, 640])\n",
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "net = lenet_decolle_model.LenetDECOLLE(Nhid=[16,32],Mhid=[64],out_channels=2, pool_size=[2,2], num_conv_layers=2, num_mlp_layers=1, lc_ampl=.5, input_shape=[2, 480, 640])\n",
    "d = torch.zeros([1, 2, 480, 640])\n",
    "print(d.shape)\n",
    "st, rt, ut = net.forward(d)\n",
    "print(rt[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f5490-a0b7-4311-80af-37368d824a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec08fa76-8a93-42a9-91b8-05eca9db7f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.4393, 0.1550]], grad_fn=<AddmmBackward0>), tensor([[ 0.0854, -0.1504]], grad_fn=<AddmmBackward0>), tensor([[ 0.3901, -0.1543]], grad_fn=<AddmmBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "print(rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "213e3376-231b-4b64-a521-57b2ca6944f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.zeros([1, 2])\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e406213-5c03-4e68-bc2f-e8fe4a8c4e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tv = decolle_loss(rt, st, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bffc1a-f0f1-44b2-b386-620613fa93ab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
