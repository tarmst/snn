{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c61e32fa-4662-4c2f-8820-130bf9fa13be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tonic\n",
    "import tonic.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from tonic.dataset import Dataset\n",
    "from typing import Callable, Optional\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b30097a0-45a9-41a5-92c4-18344bedafed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./mnist_sg_cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2b519d7-7625-47ad-bf9e-4bfea280d6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import snn_utils\n",
    "import base_model\n",
    "import lenet_decolle_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2440213f-255c-496c-8cc3-e3167d74f316",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"/media/user/EVIMO/raw/imo/eval/scene15_dyn_test_01/left_camera/ground_truth_000000\"\n",
    "sensor_size = [640, 480, 2]\n",
    "batch_size = 4\n",
    "framerate = 200\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6915dd24-0ae7-460e-a572-88ccd59905a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EVIMO(Dataset):\n",
    "#     def __init__(self,\n",
    "#                  data: np.array,\n",
    "#                  masks: list,\n",
    "#                  transform: Optional[Callable] = None\n",
    "#                 ):\n",
    "#         #super().__init__(save_to, transform=transform)\n",
    "#         self.masks = masks\n",
    "#         if transform is not None:\n",
    "#             self.data = transform(data)\n",
    "#             if len(self.data) != len(self.masks):\n",
    "#                 raise ValueError(\"Number of frames after transforms does not match number of masks\")\n",
    "#         else:\n",
    "#             self.data = data\n",
    "\n",
    "#         #target = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "#         self.targets = []\n",
    "        \n",
    "#         num_masks_include = 0\n",
    "#         for mask in self.masks:\n",
    "#             if 23 in mask:\n",
    "#                 num_masks_include += 1\n",
    "#                 self.targets.append([0, 1])\n",
    "#             else:\n",
    "#                 self.targets.append([1, 0])\n",
    "\n",
    "#         print(f\"Target present in: {num_masks_include} of {len(self.masks)} masks. {num_masks_include/len(self.masks)}\")\n",
    "            \n",
    "\n",
    "#         self.targets = np.asarray(self.targets)\n",
    "        \n",
    "#         #self.targets = torch.from_numpy(np.array(len(self.data)*[target]))\n",
    "        \n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         #return self.data[index], self.targets[index]\n",
    "#         return {\"data\" : self.data[index], \"mask\": self.masks[index], \"target\": self.targets[index]}\n",
    "#     def __len__(self) -> int: \n",
    "#         return len(self.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee4657b-59c4-4dd1-8f51-d9492c27f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVIMO(Dataset):\n",
    "    def __init__(self,\n",
    "                 dir: str,\n",
    "                 length: int,\n",
    "                ):\n",
    "        self.dir = dir\n",
    "        self.length = length\n",
    "\n",
    "        self.labels = np.load(self.dir + \"/labels.npy\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\"data\": np.load(self.dir + \"/\" + str(index) + \".npy\"), \"target\": self.labels[index]}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3cb1ea2-24d7-4457-b743-1678b5ab5fc1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Denoise removes isolated, one-off events\n",
    "# # time_window\n",
    "# frame_transform = transforms.Compose([#transforms.Denoise(filter_time=10000),\n",
    "#                                       transforms.ToFrame(sensor_size=sensor_size,\n",
    "#                                                          time_window=5000)\n",
    "#                                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d84cf6b1-6549-48cb-adbb-7511b28d4bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tonic_dataset = EVIMO(dir=\"./data/EVIMO/train\", length=703)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c71e67fd-9b03-4990-979d-cf1aa944bf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7027027027027026"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-0.2972972972972973"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26322418-e8b3-4f14-ad34-efd15c36fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frame(frame, mask): # TODO: Put the mask processing into bounding box into the tonic dataset?\n",
    "    print(mask)\n",
    "    img = frame[1] - frame[0]\n",
    "    fig, ax = plt.subplots(1,2, figsize=(10,10))\n",
    "    ax[0].imshow(img)\n",
    "\n",
    "    mask = torch.from_numpy(mask)\n",
    "    obj_ids = torch.unique(mask)\n",
    "    #print(obj_ids)\n",
    "    obj_ids = obj_ids[1:]\n",
    "    print(obj_ids)\n",
    "    detection_masks = mask == obj_ids[:, None, None]\n",
    "    #print(detection_masks)\n",
    "    \n",
    "    boxes = masks_to_boxes(detection_masks)\n",
    "    print(boxes)\n",
    "    #for box in boxes:\n",
    "    box = boxes[5]\n",
    "    ax[0].add_patch(patches.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], linewidth=1, edgecolor='r', facecolor='none'))\n",
    "\n",
    "\n",
    "    ax[1].imshow(detection_masks[5])\n",
    "    print(f\"obj 23 in mask: {23 in mask}\")\n",
    "    \n",
    "    ax[0].axis(\"off\")\n",
    "    ax[1].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#item = tonic_dataset.__getitem__(600)\n",
    "\n",
    "#print(binned_events.shape)\n",
    "#plot_frame(item['data'], item['mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7fa9a80-586b-4ace-b999-c9d0cd987723",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_trainloader = DataLoader(tonic_dataset, batch_size=batch_size, shuffle=True) # collate_fn=tonic.collation.PadTensors(),\n",
    "single_trainloader = DataLoader(tonic_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7fa7d1e-a4e4-461a-90c3-4b663b07f1bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for i_batch, sample_batched in enumerate(trainloader):\n",
    "#    print(i_batch, sample_batched['data'].size(), sample_batched['target'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f582f111-a1af-4ee6-adeb-b4bd1f13f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "datait = iter(batched_trainloader)\n",
    "batched_data = next(datait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45aaaff3-d489-42d5-9ecb-0489e2a53c0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#batched_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36bcc119-507e-4816-8847-991642b22308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_data['target'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "321a0824-3a31-4af3-b745-e4c5f7d5eee1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER SIZE: 2439712\n",
      "STDV: 0.00032011109219189804\n",
      "LAYER SIZE: 298304\n",
      "STDV: 0.0009154623036203756\n",
      "LAYER SIZE: 34048\n",
      "STDV: 0.002709718654762875\n",
      "LAYER SIZE: 512\n",
      "STDV: 0.022097086912079608\n",
      "LAYER SIZE: 64\n",
      "STDV: 0.0625\n",
      "LAYER SIZE: 256\n",
      "STDV: 0.03125\n",
      "LAYER SIZE: 512\n",
      "STDV: 0.022097086912079608\n"
     ]
    }
   ],
   "source": [
    "item = next(iter(batched_trainloader))\n",
    "data = item['data']\n",
    "target = item['target']\n",
    "\n",
    "def decolle_loss(r, s, tgt):\n",
    "    loss_tv = 0\n",
    "    for i in range(len(r)):\n",
    "        #print(f\"Loss Readout shape : {r[i].shape}\")\n",
    "        #print(f\"Loss Target shape : {tgt.shape}\")\n",
    "        loss_tv += loss(r[i],tgt) \n",
    "    return loss_tv\n",
    "\n",
    "loss = torch.nn.SmoothL1Loss()\n",
    "\n",
    "convnet_sg = lenet_decolle_model.LenetDECOLLE(out_channels=2,\n",
    "                    Nhid=[32, 64, 128, 256], #Number of convolution channels\n",
    "                    Mhid=[64, 256, 512],\n",
    "                    kernel_size=[4, 16, 32, 64],\n",
    "                    pool_size=[2, 4, 4, 8],\n",
    "                    input_shape=[2, 480, 640],  # data.shape[1:],\n",
    "                    alpha=[.95],\n",
    "                    alpharp=[.65],\n",
    "                    beta=[.92],\n",
    "                    num_conv_layers=4,\n",
    "                    num_mlp_layers=3,\n",
    "                    lc_ampl=0.5).to(device)\n",
    "\n",
    "#net = lenet_decolle_model.LenetDECOLLE(Nhid=[1,8],Mhid=[32,64],out_channels=2, input_shape=[2, 480, 640]).to(device)\n",
    "\n",
    "#convnet_sg\n",
    "\n",
    "data_d = data.to(device)\n",
    "target_d = target.to(device)\n",
    "convnet_sg.init_parameters_evimo(data_d) # Modifies readout dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e26c9fd9-1b04-4d7d-b4fc-a059bac83ce8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 480, 640])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0916f05f-2752-4d2d-bdcb-427f9ac049b9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 480, 640])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_d.transpose(0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f839e066-2097-48bc-ab39-baa0e62f7eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 480, 640])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "553f03ca-b3ef-4c1c-9662-52a31ac76679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7408e907-b4a4-4eca-8a5d-e3943bd68333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88e62e65-490a-4c02-b2ac-f5070f4280fe",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error tensor(0.5700)\n",
      "Training accuracy tensor(0.4300)\n",
      "Epoch 0 Loss tensor(1.1434)\n",
      "Training Error tensor(0.5414)\n",
      "Training accuracy tensor(0.4586)\n",
      "Epoch 1 Loss tensor(0.9750)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m st, rt, ut \u001b[38;5;241m=\u001b[39m convnet_sg\u001b[38;5;241m.\u001b[39mforward(data_d)        \n\u001b[1;32m     31\u001b[0m loss_tv \u001b[38;5;241m=\u001b[39m decolle_loss(rt, st, label_d)\n\u001b[0;32m---> 33\u001b[0m \u001b[43mloss_tv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m opt_conv\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m opt_conv\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/mambaforge/envs/snn/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/snn/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/snn/lib/python3.10/site-packages/torch/autograd/function.py:264\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m         backward_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mbackward  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    268\u001b[0m         vjp_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mvjp  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt_conv = torch.optim.Adamax(convnet_sg.get_trainable_parameters(), lr=1e-4, betas=[0., .95])\n",
    "#convnet_sg.requires_init = False\n",
    "for e in range(5):        \n",
    "    error = []\n",
    "    accuracy=[]\n",
    "    for item in iter(batched_trainloader):\n",
    "        data = item['data']\n",
    "        label = item['target']\n",
    "\n",
    "        if label.shape[0] != 4:\n",
    "            #print(f\"last entry\")\n",
    "            break\n",
    "\n",
    "        \n",
    "        \n",
    "        convnet_sg.train()\n",
    "        loss_hist = 0\n",
    "        data_d = data.to(device)\n",
    "        label_d = label.to(device)\n",
    "        convnet_sg.init_evimo(data_d)\n",
    "        readout = 0\n",
    "        \n",
    "        #for i in range(0, len(data_d)):\n",
    "            #d = data_d[i:i+1]\n",
    "            #d = torch.zeros([1, 2, 480, 640])\n",
    "            #print(f\"Data shape: {d.shape}\")\n",
    "            \n",
    "            #print(f\"Target shape: {label_d[i].shape}\")\n",
    "            #print(label_d[i])\n",
    "        st, rt, ut = convnet_sg.forward(data_d)        \n",
    "        loss_tv = decolle_loss(rt, st, label_d)\n",
    "        \n",
    "        loss_tv.backward()\n",
    "        opt_conv.step()\n",
    "        opt_conv.zero_grad()\n",
    "        loss_hist += loss_tv\n",
    "        readout += rt[-1]\n",
    "\n",
    "        #print(f\"Readout: {readout}\")\n",
    "        error += (readout.argmax(axis=1)!=label_d.argmax(axis=1)).float()\n",
    "        accuracy+=(readout.argmax(axis=1)==label_d.argmax(axis=1)).float()\n",
    "        \n",
    "    print('Training Error', torch.mean(torch.Tensor(error)).data)\n",
    "    print('Training accuracy', torch.mean(torch.Tensor(accuracy)).data)     \n",
    "    print('Epoch', e, 'Loss', loss_hist.data)\n",
    "    PATH = './EVIMO_class_cnn.pth'  \n",
    "    torch.save(convnet_sg.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9961fd94-fc2f-41db-aab0-25a5539903c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(loss_tv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ee114-ef61-4a57-9d90-f28f99147570",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "error = []\n",
    "accuracy=[]\n",
    "y_pred = []\n",
    "y_true = []\n",
    "#convnet_sg.requires_init = True\n",
    "for item in iter(batched_trainloader):\n",
    "    #print(\"Item\")\n",
    "    #print(item)\n",
    "    data = item['data']\n",
    "    label = item['target']\n",
    "    if label.shape[0] != 4:\n",
    "        #print(f\"last entry: {label}\")\n",
    "        break\n",
    "\n",
    "    #print(\"earliest label\")\n",
    "    #print(label)\n",
    "    #print(label.shape)\n",
    "\n",
    "    \n",
    "    loss_hist = 0\n",
    "    data_d = data.to(device)\n",
    "    label_d = label.to(device)\n",
    "    print(f\"data_d shape: {data_d.shape}\")\n",
    "    print(f\"data_d_tranpose shape: {data_d.transpose(0,1).shape}\")\n",
    "    print(f\"Len convnet: {len(convnet_sg)}\")\n",
    "    print(f\"data init shape: {data_d.transpose(0,1)[:, 0, :, :].shape}\")\n",
    "    print(f\"Test data shape: {data_d[0].shape}\")\n",
    "    \n",
    "    convnet_sg.init_evimo(data_d)\n",
    "    readout = 0\n",
    "    with torch.no_grad():\n",
    "        #for i in range(0, len(data_d)):\n",
    "        #d = data_d[i:i+1]\n",
    "        \n",
    "        #print(d.shape)\n",
    "        \n",
    "        st, rt, ut = convnet_sg.forward(data_d)\n",
    "\n",
    "        \n",
    "        \n",
    "        print(f\"Label shape: {label_d.shape}\")\n",
    "        print(\"Readout\")\n",
    "        print(rt)\n",
    "        print(rt[0].shape)\n",
    "\n",
    "        #print(\"labels\")\n",
    "        #print(label_d)\n",
    "        #print(label_d.shape)\n",
    "        \n",
    "        \n",
    "        loss_tv = decolle_loss(rt, st, label_d)\n",
    "        \n",
    "        \n",
    "        loss_hist += loss_tv\n",
    "        readout += rt[-1]\n",
    "        print(f\"Readout: {readout}\")\n",
    "        output = (readout.argmax(axis=1)).data.cpu().numpy()\n",
    "        print(f\"Output: {output}\")\n",
    "        y_pred.extend(output)\n",
    "        #labels = label_d.argmax(axis=1)\n",
    "        print(f\"Labels: {label_d}\")\n",
    "        labels = (label_d.argmax(axis=1)).data.cpu().numpy()\n",
    "        print(f\"Labels: {labels}\")\n",
    "        y_true.extend(labels)\n",
    "        accuracy+=(readout.argmax(axis=1)==label_d.argmax(axis=1)).float()\n",
    "        error += (readout.argmax(axis=1)!=label_d.argmax(axis=1)).float()\n",
    "        break\n",
    "        \n",
    "print('Testing Error', torch.mean(torch.Tensor(error)).data)\n",
    "print('Testing accuracy', torch.mean(torch.Tensor(accuracy)).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14e8c82-256e-490b-b93b-7f5ac7d7493d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(y_true)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2bc5a-e41a-4848-b296-e23b3c12b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "convnet_sg = lenet_decolle_model.LenetDECOLLE( out_channels=2,\n",
    "                    Nhid=[16,32], #Number of convolution channels\n",
    "                    Mhid=[64],\n",
    "                    kernel_size=[7],\n",
    "                    pool_size=[2,2],\n",
    "                    input_shape=[2, 480, 640],  # data.shape[1:],\n",
    "                    alpha=[.95],\n",
    "                    alpharp=[.65],\n",
    "                    beta=[.92],\n",
    "                    num_conv_layers=2,\n",
    "                    num_mlp_layers=1,\n",
    "                    lc_ampl=.5).to(device)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685f938a-2daa-4af3-98e2-ddf8be8f91b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = lenet_decolle_model.LenetDECOLLE(Nhid=[16,32],Mhid=[64],out_channels=2, pool_size=[2,2], num_conv_layers=2, num_mlp_layers=1, lc_ampl=.5, input_shape=[2, 480, 640])\n",
    "d = torch.zeros([1, 2, 480, 640])\n",
    "print(d.shape)\n",
    "st, rt, ut = net.forward(d)\n",
    "print(rt[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f5490-a0b7-4311-80af-37368d824a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec08fa76-8a93-42a9-91b8-05eca9db7f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e3376-231b-4b64-a521-57b2ca6944f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.zeros([1, 2])\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e406213-5c03-4e68-bc2f-e8fe4a8c4e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tv = decolle_loss(rt, st, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bffc1a-f0f1-44b2-b386-620613fa93ab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
