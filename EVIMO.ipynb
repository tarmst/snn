{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c61e32fa-4662-4c2f-8820-130bf9fa13be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tonic\n",
    "import tonic.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from tonic.dataset import Dataset\n",
    "from typing import Callable, Optional\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import matplotlib.patches as patches\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b30097a0-45a9-41a5-92c4-18344bedafed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./mnist_sg_cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2b519d7-7625-47ad-bf9e-4bfea280d6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import snn_utils\n",
    "import base_model\n",
    "import lenet_decolle_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb4260d3-1022-4b5c-816d-d15b6322f461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.cuda.device at 0x7f6028f011e0>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[torch.cuda.device(i) for i in range(torch.cuda.device_count())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2440213f-255c-496c-8cc3-e3167d74f316",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"/media/user/EVIMO/raw/imo/eval/scene15_dyn_test_01/left_camera/ground_truth_000000\"\n",
    "sensor_size = [640, 480, 2]\n",
    "batch_size = 4\n",
    "num_bins_per_frame = 10\n",
    "framerate = 200\n",
    "device = 'cuda'\n",
    "epochs=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee4657b-59c4-4dd1-8f51-d9492c27f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVIMO(Dataset):\n",
    "    def __init__(self,\n",
    "                 dir: str,\n",
    "                 item_to_find: int,\n",
    "                 num_bins_per_frame: int,\n",
    "                 #start_idx: int,\n",
    "                ):\n",
    "        self.dir = dir\n",
    "        self.item_to_find = item_to_find\n",
    "        self.num_bins_per_frame = num_bins_per_frame\n",
    "        self.length = np.load(self.dir + \"/length.npy\")\n",
    "        print(self.length)\n",
    "        #self.start_idx = start_idx\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = np.load(self.dir + \"/\" + str(index) + \".npy\", allow_pickle=True).tolist()\n",
    "        if self.item_to_find in item[\"objs_in_mask\"]:\n",
    "            target = np.asarray([0, 1])\n",
    "        else:\n",
    "            target = np.asarray([1, 0])\n",
    "\n",
    "        target = np.tile(target, (self.num_bins_per_frame, 1))\n",
    "\n",
    "        return {\"data\": item[\"binned_events\"], \"target\": target}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length # - self.start_idx\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d84cf6b1-6549-48cb-adbb-7511b28d4bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477\n"
     ]
    }
   ],
   "source": [
    "tonic_dataset = EVIMO(dir=\"./data/EVIMO/left_cam/scene13_test5\", item_to_find=23, num_bins_per_frame=num_bins_per_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c71e67fd-9b03-4990-979d-cf1aa944bf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7027027027027026"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-0.2972972972972973"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac698a22-7570-4b2e-81cf-0b6d506e90e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tonic_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7fa9a80-586b-4ace-b999-c9d0cd987723",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_trainloader = DataLoader(tonic_dataset, batch_size=batch_size, shuffle=False) # collate_fn=tonic.collation.PadTensors(),\n",
    "#single_trainloader = DataLoader(tonic_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7fa7d1e-a4e4-461a-90c3-4b663b07f1bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for i_batch, sample_batched in enumerate(trainloader):\n",
    "#    print(i_batch, sample_batched['data'].size(), sample_batched['target'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f582f111-a1af-4ee6-adeb-b4bd1f13f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "datait = iter(batched_trainloader)\n",
    "batched_data = next(datait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45aaaff3-d489-42d5-9ecb-0489e2a53c0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 2, 480, 640])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_data['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36bcc119-507e-4816-8847-991642b22308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_data['target'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "321a0824-3a31-4af3-b745-e4c5f7d5eee1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER SIZE: 2439712\n",
      "STDV: 0.00032011109219189804\n",
      "LAYER SIZE: 298304\n",
      "STDV: 0.0009154623036203756\n",
      "LAYER SIZE: 34048\n",
      "STDV: 0.002709718654762875\n",
      "LAYER SIZE: 512\n",
      "STDV: 0.022097086912079608\n",
      "LAYER SIZE: 32\n",
      "STDV: 0.08838834764831843\n",
      "LAYER SIZE: 64\n",
      "STDV: 0.0625\n",
      "LAYER SIZE: 128\n",
      "STDV: 0.044194173824159216\n"
     ]
    }
   ],
   "source": [
    "item = next(iter(batched_trainloader))\n",
    "data = item['data']\n",
    "#target = item['target']\n",
    "\n",
    "def decolle_loss(r, s, tgt):\n",
    "    loss_tv = 0\n",
    "    for i in range(len(r)):\n",
    "        #print(f\"Loss Readout shape : {r[i].shape}\")\n",
    "        #print(f\"Loss Target shape : {tgt.shape}\")\n",
    "        loss_tv += loss(r[i],tgt) \n",
    "    return loss_tv\n",
    "\n",
    "loss = torch.nn.SmoothL1Loss()\n",
    "\n",
    "convnet_sg = lenet_decolle_model.LenetDECOLLE(out_channels=2,\n",
    "                    Nhid=[32, 64, 128, 256], #Number of convolution channels\n",
    "                    Mhid=[32, 64, 128],\n",
    "                    kernel_size=[4, 8, 16, 32],\n",
    "                    pool_size=[2, 4, 4, 8],\n",
    "                    input_shape=[2, 480, 640],  # data.shape[1:],\n",
    "                    alpha=[.95],\n",
    "                    alpharp=[.65],\n",
    "                    beta=[.92],\n",
    "                    num_conv_layers=4,\n",
    "                    num_mlp_layers=3,\n",
    "                    lc_ampl=0.5).to(device)\n",
    "\n",
    "#net = lenet_decolle_model.LenetDECOLLE(Nhid=[1,8],Mhid=[32,64],out_channels=2, input_shape=[2, 480, 640]).to(device)\n",
    "\n",
    "#convnet_sg\n",
    "\n",
    "data_d = data.to(device)\n",
    "#target_d = target.to(device)\n",
    "convnet_sg.init_parameters(data_d) # Modifies readout dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e26c9fd9-1b04-4d7d-b4fc-a059bac83ce8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 2, 480, 640])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0916f05f-2752-4d2d-bdcb-427f9ac049b9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data_d.transpose(0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f839e066-2097-48bc-ab39-baa0e62f7eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "553f03ca-b3ef-4c1c-9662-52a31ac76679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7408e907-b4a4-4eca-8a5d-e3943bd68333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88e62e65-490a-4c02-b2ac-f5070f4280fe",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930bc42e752a4b7b989c8bbeb4427c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m convnet_sg\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     15\u001b[0m loss_hist \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 16\u001b[0m data_d \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m label_d \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m convnet_sg\u001b[38;5;241m.\u001b[39minit(data_d, burnin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt_conv = torch.optim.Adamax(convnet_sg.get_trainable_parameters(), lr=1e-4, betas=[0., .95])\n",
    "\n",
    "for e in range(epochs):        \n",
    "    error = []\n",
    "    accuracy=[]\n",
    "    for item in tqdm(iter(batched_trainloader), desc=f\"Epoch {e}\"):\n",
    "        data = item['data']\n",
    "        label = item['target']\n",
    "\n",
    "        if label.shape[0] != 4:\n",
    "            #print(f\"last entry\")\n",
    "            break\n",
    "        \n",
    "        convnet_sg.train()\n",
    "        loss_hist = 0\n",
    "        data_d = data.to(device)\n",
    "        label_d = label.to(device)\n",
    "        convnet_sg.init(data_d, burnin=2)\n",
    "        readout = 0\n",
    "\n",
    "        data_d = data_d.transpose(0, 1)\n",
    "        label_d = label_d.transpose(0, 1)\n",
    "        \n",
    "        for n in range(num_bins_per_frame):\n",
    "            st, rt, ut = convnet_sg.forward(data_d[n])   \n",
    "            #print(rt[0].shape)\n",
    "            #print(label_d.shape)\n",
    "            loss_tv = decolle_loss(rt, st, label_d[n])\n",
    "            \n",
    "            loss_tv.backward()\n",
    "            opt_conv.step()\n",
    "            opt_conv.zero_grad()\n",
    "            loss_hist += loss_tv\n",
    "            readout += rt[-1]\n",
    "\n",
    "        #print(f\"Readout: {readout}\")\n",
    "        #error += (readout.argmax(axis=1)!=label_d.argmax(axis=1)).float()\n",
    "        #accuracy+=(readout.argmax(axis=1)==label_d.argmax(axis=1)).float()\n",
    "\n",
    "        error += (readout.argmax(axis=1)!=label_d[-1].argmax(axis=1)).float()\n",
    "        accuracy+=(readout.argmax(axis=1)==label_d[-1].argmax(axis=1)).float()\n",
    "        \n",
    "    print('Training Error', torch.mean(torch.Tensor(error)).data)\n",
    "    print('Training accuracy', torch.mean(torch.Tensor(accuracy)).data)     \n",
    "    print('Epoch', e, 'Loss', loss_hist.data)\n",
    "    PATH = './EVIMO_class_cnn.pth'  \n",
    "    torch.save(convnet_sg.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9961fd94-fc2f-41db-aab0-25a5539903c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(loss_tv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ee114-ef61-4a57-9d90-f28f99147570",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "error = []\n",
    "accuracy=[]\n",
    "y_pred = []\n",
    "y_true = []\n",
    "#convnet_sg.requires_init = True\n",
    "for item in iter(batched_trainloader):\n",
    "    #print(\"Item\")\n",
    "    #print(item)\n",
    "    data = item['data']\n",
    "    label = item['target']\n",
    "    if label.shape[0] != 4:\n",
    "        #print(f\"last entry: {label}\")\n",
    "        break\n",
    "\n",
    "    #print(\"earliest label\")\n",
    "    #print(label)\n",
    "    #print(label.shape)\n",
    "\n",
    "    \n",
    "    loss_hist = 0\n",
    "    data_d = data.to(device)\n",
    "    label_d = label.to(device)\n",
    "    print(f\"data_d shape: {data_d.shape}\")\n",
    "    print(f\"data_d_tranpose shape: {data_d.transpose(0,1).shape}\")\n",
    "    print(f\"Len convnet: {len(convnet_sg)}\")\n",
    "    print(f\"data init shape: {data_d.transpose(0,1)[:, 0, :, :].shape}\")\n",
    "    print(f\"Test data shape: {data_d[0].shape}\")\n",
    "    \n",
    "    convnet_sg.init_evimo(data_d)\n",
    "    readout = 0\n",
    "    with torch.no_grad():\n",
    "        #for i in range(0, len(data_d)):\n",
    "        #d = data_d[i:i+1]\n",
    "        \n",
    "        #print(d.shape)\n",
    "        \n",
    "        st, rt, ut = convnet_sg.forward(data_d)\n",
    "\n",
    "        \n",
    "        \n",
    "        print(f\"Label shape: {label_d.shape}\")\n",
    "        print(\"Readout\")\n",
    "        print(rt)\n",
    "        print(rt[0].shape)\n",
    "\n",
    "        #print(\"labels\")\n",
    "        #print(label_d)\n",
    "        #print(label_d.shape)\n",
    "        \n",
    "        \n",
    "        loss_tv = decolle_loss(rt, st, label_d)\n",
    "        \n",
    "        \n",
    "        loss_hist += loss_tv\n",
    "        readout += rt[-1]\n",
    "        print(f\"Readout: {readout}\")\n",
    "        output = (readout.argmax(axis=1)).data.cpu().numpy()\n",
    "        print(f\"Output: {output}\")\n",
    "        y_pred.extend(output)\n",
    "        #labels = label_d.argmax(axis=1)\n",
    "        print(f\"Labels: {label_d}\")\n",
    "        labels = (label_d.argmax(axis=1)).data.cpu().numpy()\n",
    "        print(f\"Labels: {labels}\")\n",
    "        y_true.extend(labels)\n",
    "        accuracy+=(readout.argmax(axis=1)==label_d.argmax(axis=1)).float()\n",
    "        error += (readout.argmax(axis=1)!=label_d.argmax(axis=1)).float()\n",
    "        break\n",
    "        \n",
    "print('Testing Error', torch.mean(torch.Tensor(error)).data)\n",
    "print('Testing accuracy', torch.mean(torch.Tensor(accuracy)).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14e8c82-256e-490b-b93b-7f5ac7d7493d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(y_true)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2bc5a-e41a-4848-b296-e23b3c12b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "convnet_sg = lenet_decolle_model.LenetDECOLLE( out_channels=2,\n",
    "                    Nhid=[16,32], #Number of convolution channels\n",
    "                    Mhid=[64],\n",
    "                    kernel_size=[7],\n",
    "                    pool_size=[2,2],\n",
    "                    input_shape=[2, 480, 640],  # data.shape[1:],\n",
    "                    alpha=[.95],\n",
    "                    alpharp=[.65],\n",
    "                    beta=[.92],\n",
    "                    num_conv_layers=2,\n",
    "                    num_mlp_layers=1,\n",
    "                    lc_ampl=.5).to(device)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685f938a-2daa-4af3-98e2-ddf8be8f91b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = lenet_decolle_model.LenetDECOLLE(Nhid=[16,32],Mhid=[64],out_channels=2, pool_size=[2,2], num_conv_layers=2, num_mlp_layers=1, lc_ampl=.5, input_shape=[2, 480, 640])\n",
    "d = torch.zeros([1, 2, 480, 640])\n",
    "print(d.shape)\n",
    "st, rt, ut = net.forward(d)\n",
    "print(rt[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f5490-a0b7-4311-80af-37368d824a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec08fa76-8a93-42a9-91b8-05eca9db7f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e3376-231b-4b64-a521-57b2ca6944f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.zeros([1, 2])\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e406213-5c03-4e68-bc2f-e8fe4a8c4e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tv = decolle_loss(rt, st, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bffc1a-f0f1-44b2-b386-620613fa93ab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
