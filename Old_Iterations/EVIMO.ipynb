{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c61e32fa-4662-4c2f-8820-130bf9fa13be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tonic\n",
    "import tonic.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from tonic.dataset import Dataset\n",
    "from typing import Callable, Optional\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import matplotlib.patches as patches\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b30097a0-45a9-41a5-92c4-18344bedafed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./mnist_sg_cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2b519d7-7625-47ad-bf9e-4bfea280d6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import snn_utils\n",
    "import base_model\n",
    "import lenet_decolle_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2440213f-255c-496c-8cc3-e3167d74f316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = \"/media/user/EVIMO/raw/imo/eval/scene15_dyn_test_01/left_camera/ground_truth_000000\"\n",
    "sensor_size = [640, 480, 2]\n",
    "batch_size = 4\n",
    "num_bins_per_frame = 16 # T = 100\n",
    "framerate = 200\n",
    "epochs=20\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eee4657b-59c4-4dd1-8f51-d9492c27f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVIMO(Dataset):\n",
    "    def __init__(self,\n",
    "                 dir: str,\n",
    "                 item_to_find: int,\n",
    "                 num_bins_per_frame: int,\n",
    "                 #start_idx: int,\n",
    "                ):\n",
    "        self.dir = dir\n",
    "        self.item_to_find = item_to_find\n",
    "        self.num_bins_per_frame = num_bins_per_frame\n",
    "        self.length = np.load(self.dir + \"/length.npy\")\n",
    "        #print(self.length)\n",
    "        #self.start_idx = start_idx\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = np.load(self.dir + \"/\" + str(index) + \".npy\", allow_pickle=True).tolist()\n",
    "        if self.item_to_find in item[\"objs_in_mask\"]:\n",
    "            target = np.asarray([0, 1])\n",
    "        else:\n",
    "            target = np.asarray([1, 0])\n",
    "\n",
    "        #target = np.tile(target, (self.num_bins_per_frame, 1))\n",
    "        #print(item[\"events\"].dtype)\n",
    "        \n",
    "        events = np.asarray(item[\"events\"])\n",
    "        #target = target\n",
    "\n",
    "        frame_transform = transforms.Compose([# transforms.Denoise(filter_time=0.01),\n",
    "                                       transforms.ToVoxelGrid(sensor_size=sensor_size,\n",
    "                                                          n_time_bins=self.num_bins_per_frame)\n",
    "                                      ])\n",
    "\n",
    "        # #transform = transforms.ToFrame(sensor_size=sensor_size, )\n",
    "        bins = frame_transform(events)\n",
    "\n",
    "        #return {\"data\": events, \"targets\": target}\n",
    "        return bins, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length # - self.start_idx\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d84cf6b1-6549-48cb-adbb-7511b28d4bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tonic_dataset = EVIMO(dir=\"./data/EVIMO/left_cam/scene13_test5\", item_to_find=23, num_bins_per_frame=num_bins_per_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a971906-eab8-4439-b710-1fe338eb8c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 1, 480, 640)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins, target = tonic_dataset[0]\n",
    "bins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3217b81a-5a5c-4ae5-a051-9f7690e027e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAE2CAYAAACtJt9GAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAps0lEQVR4nO3da3MbWXof8P85p3EhIIA3USR1GVF3jW4jja6zF9s73vXa3k3Wu87YiV3xi8Spiv0J8j4fIFXJO1eSqjjl1Mbj2N7EXnu9u/aMPTujmZFG0qw0Gl1Gd+pGkZRIAiSI7nPyAiJ4A0mAaPTpRv9/VVsDgo3uZykSD87pc55HGGMMiIiIfCRtB0BERO2HyYWIiHzH5EJERL5jciEiIt8xuRARke+YXIiIyHdMLkRE5DsmFyIi8p1T74HfkG+1Mo62J44dhBqfhHvnXl3HOzu2w719FzKdhp4tA9qrnCeRhEinoCcnWxkuke9EIonyVw/D+fvztkOhJv1Yv73mMRy5BMRcuFJ3YgGAsTODEIkksH8nYPT8ecqzTCwUScbzkLo/bjsMCoiot/wLRy4tIBWEFDDaANqDTKcBAHpmxnJgREQrq2fkUve0GPlLZrMQm/uhu7IQWsOcv8KkQkRtI7jkIgRUVxe8cQ6LAUAXCsCNW7bDICJqieDuuRjDxELxJoTtCIgCE/gNfWfrlqAvSTRPKohUys6lD++zcl0iGwJPLqZcDvqStITMZm2HYI/RgA62hZHq7YH50msQj0YDvS5RM1RvDyZ/+8y6Xx94cvGePA36krSE2DJgOwR7jIEpz7bs9KVfPwnV27PoOXffNoizl+GNjLTsukR+80bHkPvfZ9f9eu5ziSHv+he2Q2graveOyp4kAJkPv4A3/mLR98X7l6qbYIniInbJReXz8H7pddthUNQIAeE4UPn88m8tmGbzRseYSKj9CAHV3d3QS9ozuayyKsebmIB690KAwVA7EE4CMpeDe3jnsu+5t+60dKqNaBkhoL9yNLjrrWO1r7XkInO5lqzaEakUxn+vchNKbeyFzOWWH1RfUQKKMZXPwxkcgLNtK4BK2R1vfBziZxftBkYEAMYgcbX+clI1CQG1e4c/8dRgLbnoyUmYUsm388lstlI+RRt0/48PMPPPTgE9XRDcW0BrkJkMZDqN4ndPwxnoB1AZ4bqPHsO9/8BydES1eaNjzZ8kmWj+HCuwWltMOA5EMgldLNZ1vMrn4U1M1Pye/spROFOzkM+nKgUipeLcNzVEOA6M69oOgyj0WloVWabTcAabW9JqPA96pv7Ry8IkpHp7Ft1bke9dhL742Xzl4ToSi8xm4f7y8foDprYWZGIRxw4Gdi0C1MF9DVdIcAYHoPr6WhRRsGa/eQJq765Ar7nu5CKSSZgNmeaubgxkNrPoH10d2LtoHlAe2V/9/sI/fm90rOl7J7pQgPPT+PaWkNlsdQktBctcuGI7hFgpb8wAorG3O7MhA5HtaFFEwUr+6FzgWxDWXbjSm5gAVpiiasTkNw8g96PPqj1KvM+/WNS/RF++wRvwLaILBdshELWcs2UzcHcUboPT5B4Lyzal4eSi9uyEvn3PtymE7J99CL3wiaW/ALxvQtRWnIF+lHcOVDaX+khmMoDW0DMzUK/ugUklICeK0E9GIJTy9Vq1qN4eiHwO7u27Lb9WFDQ8LTa7pQvCYRsYIlof9/ET3xMLAIiONESyMs3rXb0BcesByoNd0IXCiguB/OSNjjGxLNBwclHvfAI9MwNxnDckiSg4IpGE6uqEfO3VyteOA+9r89U2vNGxRUnEm5jgviSL1n1DXz19sfZBRDXMlVBRfX1swUCrkof2Q//iscrjbAe8iSnIZy8w9m/egOzMI/mw9SMSWp91z29xcxnVSzgOjDbV+2eitxuYmGCVYKpJdXXCe/4CEAJysoCE1vAAiJ5u4PkLuMMP0fPfH8IDAD82ElJLtGdtMbJmrlzKQub1VyGPzDfK4rw0LeVs3VIZoQgB70VlNOLs2A4zMQV94w6ASg03ig7emSffuG8ex1Svg8ThQaR++PH8Nz76+eIVgUQvOVs2Y+LUNuR+/hSJZ0V4C7YdMJmEj7N9G9y79+s7tsWxUMStVhJF7dsN78bt6nRXYqKE9K2R+SoJRGtwhx8i8xcvp7go9KaODCJdZ3LhtNgSMp22HUJ4SIXx3zlZ6WWSSkFt7F1U8kffub9oH5I5d5mJhaiNpf/fR3Ufy5HLEnXW8YwH7aHrjz+olN/RBnBdmAUlNPysak1E7YXJZQm+Ydbwsu+895wNsYioPuuvipzJ1FwZREREtP57LkIAko24iIhouXUnF10o1L0kjYjCpZXtbYkArhYjagurNrUSolqPa455+KTFEVHcMbmsRqpAlyaLVIpLoWldRDIBCAHV1bn8e04Cw1/vXvRcva3FidaLyWUVqrcH2L8zsOuZUgl6Ziaw61H7cIcfQqZSmHxz/7LvmfIsBv/ThxaiojhjclmFNzICffGzYC8qFcSxg5DZbLDXpchQB/YCQsAZemXR83pmBpk/XyGJsOkeBYzJpQ6tbo4mczlMf+cUpn7rDIQUKPVnINKpyoo8irclvwMikcTM5hwgJHRn+D6AiETSdggUEkwuaxECctdQSy/x4lsHkf3Rp8j94AKM6yL5tx/DGx2DyuVael0KP5nJAHK+Ra8pzyLxk/OA9qAvXbUYWW1yzxCASitjmcnU9yIhIE4cal1QZAWTy1qMgXft5rpfrnp7Kp8+Tx2uPK4h9/2z0DMzy6oDBNGalcJNFwrRmtIaGYfq7oaenIKZrV3RQR5Zcl/IGKjhZwEEtzK1b7fV67cjJpcWUL09cLZvq3zR3QmhFISn4bGxEbW77jxEVx66UFixmrb+9PPq47kl1O6jx4GEt5LpHd1rH0QNEabOSo3fkG+1OpbIUd3dgBTw9myFuvFgPnkIAQgZrU+cRDYIAbBYbOT8WL+95jEsXNkgmU7DGANTKsFMTwNSwnnyAnpmwZSWMYBhYiFaUxCJZe6eFT/sBYrJpUFiQxbCGHgL9qRotu0lCi2RcCCEgJ5hcgkSk0uDvGejtkMgogaYUgmceAseb+gTveRs28p9GkQ+YXIhQmWj7OTrmyE7ubeIyA+cFqNYcgYHoPu6oD/9HM6WzYDW6PjBR+CsPJE/OHKh0FJdnTV3eauD+6D27qr7PLXqtBmtIcqVVOIOP7S+z6LdiVQKKp8HAKh8HmrP8oKwzuAAVF8fpybbBJMLhZZ7YAhic/+y570r1+Bd/2LRc87OoWVJRH/1GEQiCbF9y/JzPHkK7+oNfwOmFZlSqVpxwpuYgHfj1rJjXnx5O6aPDy2ampz6rTOBxUj+8jW5qL6+1ZsW+UicONTygpJkl9fhAMlE9WuZy9UcsTiDA5jZ0QuZ2wBIVS2zk/j5LRi3DO+z64HFTOuX/bMPK3X1FqzI7ProIeQ6a+zVXduMWsLfd2cpAB3Moj8jBVgzuL05Pz1fvQcis1mgXK6OWFR3N8pHhiDfvVDZyPrT85grNjJXKcF7/iL4oMlX7p17gV9T5fPAQN+y0TE1huVfIkRmMjBlF6ZcuyBgO3MG+mFKs/DGx+efZOkQWif9laNI3hmB+2DYdiiRVE/5l0jfc1FdnRCplO0wAqNnSjBu2XYYVnjjz6EnJxc/ycTSWkJAbey1HYUvRCoFeWg/Zr59CgCQ+Pw+9LNRyCP7Ob3eIpFOLkgkIZRa+7h2ob3YvqGaUmnFKru0fqt+ODNm0f0P/ZWji8vlC7Hyyi6bje6EwOyvnlz0lCmVoC9/jvRffQSgUmlDz8zA25CqFJld8nqRSLJZX5M4LUYUEzKXA3ZuXdRkTB7aD33581VetfAECjC6+gHHGeiHyWUhZmYBJefvjwgB1Zm3es9LOM66P4zIbBayvw/68VNA62oNQZrX9tNiRFQ/Mz0NMfx00XN1JxZg2cjZGx2Hd+MW9Ng49OiCe2HGwHv+AqO//wZkJuPr1Nrz33ujrlVgzYxydaEA99Yd6GKRiaUJTC60qpW6Z1J4lX7tZM3njev6Wnh1bmGJLhSW3w8D0PtfPwCE8PWaXX/8AXSx6Nv5CJVpwBbcd2JyoWXmdlIDgCkUY7VoIuwW/tusJP2TSwFEUh9dKNgOgdYgMxnIoW3+n9f3M1KkOYMDMN58hS1jDMS+HZV59P5NFiOLt7kRpOhcO7m0w1J1lc+j/CsnbIcRC7pQgHfztu/nbY/k0kZLJm0QjlP9Q9b9PYC3uHxjYUflDc178nTZa6kFpMLsN5e8sXZ3AgDc+w8sBBQ8b2ICib87t/wbL//Wa039CcfhsuIQaZ/VYtxQ1xSZTvPmZYjw32MVQkCmUst+PubLR6GmSotWw1FrxGu1mDFQXZ22o4gEZ+dQ9bE4cQgA+EYWMvz3WIUxNX8+4mcXrSSWhe87IpVC6ddPVh+P/ts3Ao8nLNonuQDApo22I7BG9fbAfOm1VY8RiSTUwX2YPDx/78Scu9zq0IjaW19vZQ8QKps1Uz/8uPq49799YDMyq9oqucS50Jw3/gLqk2tQXZ3VT1LO9sUrQEx5FvrmHXT84CMbIRJFwtK/m7Xoe8MwZw61KJrgOYMDvvTUaavkElfO4ADUnh1wT7/6srBlZQOZmSktO9aUlj9HDRKi+kmV2k+tvxuRSMLZtvXlF0v+/bWBmmifvyszO1upxNAkJpc2YLIdEMUZyHcvVDa0vdxbwNVd/pp66zRkOg3hJCA70rbDoRap9XcjN2QxcWILIBVEMgmZnt/7ZcqzjVU6CDlvdMyXOn7+rBaTCjKd4s5ZahvyyH6I+08WlfhXvT3wxsa5KpFCR/VvCvTDZMtXi6mNvfPF7HTzwyiisBCuXrbfxxsdY2Iha1T/phWnY1ueWKRqeBN1U8nFezZaLWYX+6WTLM/dVrzPrld7vhOFwdQbQ5DZgFo3L3k/k9kMpt4YaugUvOfik2f/7oztEIiojXX85Uc1C4T6TgiM/v7i9zM9OYmOv2xslSmTi082/lF817MTURsxBpv+4nrTp2nr5FL6tZPctU9E1AghMP4re5o+TVsnl+ynw9BTrSv5LY8eqD4Wxw7C+9rrLbsWEVEgjEH331xruhhwWycXd/ihL+u1ZS5X7Wkis1nIbBYQAuWu+b0O5sIVqH/4pOlrkT9kOj3f+2SNZkgymw0oKqLWmvidM3B2bG/6PCKVbLqPU1snl2YJx4HK5zH9C/shdw8BwPwmRWOg3mEyCSs9M1Nd7aU29UHs3137QCFQ+vKrAUZG1DqJogbKzX+gdh8/gTv8sKlzxD65iFQKqq+vWinY2bYVOHMEAGA8D7pYROqvP4Z35ZrFKKlR4uTh6mPv6QjM5zdrH2hM7b4hRBGj+vowm5VwHwzbDgVAO/VzWYNwnPkpMiEgj+xHuacDqTujcG/ftRtcnAkBmcmwHS5RhMSrn8sqZDaL5799ojoHr7q6gBt34bx3Ge6de3aDiznVmcfEtw6vfSARRUoseoLqQgG97w3De1kReGG9KLLLe/4CG/70rO0wiMhnsRi5AIB79z7LzVsk06wiTBQnsUku1BryyP5FBe1kOl1zffzz7x1l/TVqe+bLR5tewtsumFyoKXJ8CpiuFC31vvY6sHsIpjS77Lj89z+G3LAh6PAoAGr3Dqh9Kyz1jhlnZHJZNe24isU9F1rM+6XXod694Ev5ePf+g+pj9Q+fYKXGCyLhQPb1Qk9NsWx9m/Fu3rYdQmjEudX6Uhy5xFBy+Dnk4X2BXtOUSnBv3WFiIYoJJpcY8m7cgv60fdqyLvTid89AJJK2wyCKPU6LUVvp/JOz4NiIyD6OXNpEFFeoNNo2lYiig8kl4lRvT6Wvto7g53VtKr251yjtrTb2rtg7nOySuRz/bQCors5IfsBrJSaXiJs+uQsym4EpL1/+G1Yym630vxkZgcxmMH1i56rHF0/uDK53ODVm1zb+2wBwD+6A2tRnO4xQiU3hyigSiSTkjm2LlzdKBZlMQM/M1HyNMzgAAHAfPW7gQgLO0CvBFfCUCmpDtloSnyjKhONAbd0MCIHZLd2Q7120HVLLsXBlxJny7PJ189pbMbEAAByn8r9GCAndubxh1vR3TtW8L+IMvdJUg63SN1+HN1XAi989s+j5Rs6p+jdh+junGroum4KRL6SCSCSrLdRlLocHv7EV+tETJC5zz8+cyIxcnMEBIJVkFeMgSQXZkfa1HL5wHMh9u+B9dh1CKQjHqSZL86XXID+6Un/3UKkAXd9uaOE40KcOQrx/ab2hEwEA1Kt7MNufg5tRSP3w48qTDfwutoN6Ri6RSS4ynQakhC4WrcZRL+E4kDu3c8fuWmL2R0nRF7X3olaoJ7mEb5+LEDV3ca86FRRCxnWZWOrBxEIRE7X3IltCd89FdnTYDoGI1qD27YbavcN2GBRioUguzkA/1MF9KP36SYhstrJ3wyapoPZUlseKRHLZjWeiuHK2bMbMt08BT0eBZ2O2w6EQC8W0mOnOQ7yYQuqH14A9OyHKLjC6+BdXpFKBNfsSSqHcn4e8UVmx1fkn7JRIBADu8EOkhx+Ck5m0llCMXLyrN+A+GK48vnGr5oqw8pcPtTQGmctVH5vybCzWqhMRtUookks9nL8/39Lz68nJlp6fiChOIpNciIgoOphcKPJkOg0AcLZuWfEYdWAvRKOVC2g5IVigkerC5EKRZ7xKc2UzPb3iMWKiABPFytFhxB7xVAd+lKPIm6sI7Y2uvDR2bsEINcmY+svzUKxx5EJtSR3YazsEoliznlycgX42GyLflXsykK+9CgCQGfYbIQqa9eRSPPoKZEfadhjUZuR7F6EvXQUAiM39lqMhih+ryUWkUuj48KavJd2JlvJusscGUdCsj1xgtO0IiIjIZ1ZXi5lSCV5A9cKIiCg49kcuRA1yBvqh+vpsh0FEq+A+F4oc9/ET2yEQ0Ro4cqHIkdms7RCIaA1MLhQZMpuFs3MosL4+VCESSah83nYYFDFMLhQZIpmE15ll+ZGACSUBFqukBjG5UKgtnALzxsdhLlyxGE08iY4OoKfTdhgUMUwuPnr+r9+ASCRth9EWRCKJ8tePcwosBHSxCDxbuSgoUS3CGFNXHfJvyLdaHQsRhY0Qlf/W9zZBMfFj/faax3DkQkQrcvo3QeVytsOgCOI+FyJaEfcU0Xpx5FIHtWfnqi10iag9qf5NEMcPVh7v3mE5mmhhclmFzOXgDL0Cffse3IePbYdDdeCCCvKLs3MIw/9yN8SVLwAA+s59yxFFi9XkInM5ONu22gxhdeUyTGG6sq9Cs294JLDKtm+cwYF4N/IrTmPLn1yDnpkBAO6vapDV5KInJ+Hef1D5Ym5VSojomRl4IyO2w6AG8A3APzMHtkAmE7bDsMZ9/ATes1HbYSyj+jfBffN4Yy+y8P4ammkxtXeX7RCIaAHnp+ern9opPPToGJIfX6/7eOE4ULuGWhfQCkKTXLxrN6uPnQG2pSUiqsW4LvTkZEPH2+jGGprkstDUie2hnCYjIqL6hHKfS/qvPrIdAhERNSGUIxciIoo2JhciIvJd8MlFiHivnSeKAJXPsyoFNcXKyEVI3qyndZIK3tdeh3AcyEzGdjRty5uYgPtg2HYYFGHBJxdjuNGN1k97UO9egkgmoYtFyHQaAGC+9BrUxl7LwRHRnMCTi7NlM+SR/UFfltqJ9ioNrIRA6RcOQTgOnC8eQU9O2Y6sLaiNvRwVUtMCX4rsDj8Ehh8GfdllZC7X0EYkCp5IJGE8b+W6bsYg8XfngEQS3pOnwQbXxkQ2A7guULQdCUVZbFeLTfzaQdsh0Brk7u1w+vvWPM6UZwOIJj7cu/fhPX9hOwyKuFBuogzChj89azsEWoN39YbtEIhonWI7cqH6sUcKETUqfMlFKu6DCRnvzEHWeiOihoRvWozNnkJH/tMF2yEQUcSEMLkY2xEQEVGTwjctFlH6K0dhvnyUU3oUaXObUomaxeTSJNXbg+J3T8N5PoNSTxIyO7/5TCSSNZONM9CPkT94o+a5VN/aS2+JWoWdJ2k1as9O6F88Vtexwpj65qG+Id9qKqi6SIXpf34cHX/Z3v1chFOZjVyrDI5wHKitm+HeuVf/uRNJwOhl51a7d0B3ZWHOXa5c+43XIM9d5R4RImrYj/Xbax4Trnsu2sOGd29ghf3YoSJSKZjZ2XXdI6q3tppxXXjDj+s/sVS49x9OoOOpwcY/OluJTQjIVAr6/kOIhwpz0ZbzCaRWKSAq02nIgU0wCQfejVv1x0BEhLAlFwDe+LjtEOpiSqVgrtPIyEJ72PYf319yAlOd6liYBpM/OofV0qIxBnA9cAEyEa0H77k0wP3l41D5vO0wAmFKJbgPhhuakmuGzOXYP4SojTC5NCB18Ta8qYLtMNqSLhThjTyzHQbRImzjsH5MLg3wRsdWrtBLzdFeQ1ON3tdeZ9UAohALLLnITAblrx8HhKiuliJaL/XOBW64pdaSCt7Yc9tRRFZwIxch4GYVO1GSP5hYyA9CQPX2LHrKffN45YFepZcQrSmw5KILBXT8oPb+FZFI4sXvngkqFCKiKpFILJpidf7+vMVo1k9msxCpFABg9ldPovCbp63GE4r5KTm0Fb3vDYPjGSIKlDFwHz+xHYUvvNd2Q5ytbJJO/eQCUsCq2w1aLRTJhZv0qG5CQO3ZCe/6F7YjIQoV8f6l6uMw3HrgajGKHLcvF9i1Cr95ms3SiNaByYWixRiIn10M7HL5927DO30gsOsRtQsmF6JV6OcvkLw/CqBS/HNpleuJ3+FCFKJamFwoUsSJQ5DZbGDXM6US3Lv3AQDezdvLlqbm/9dZiERyxfYKVFvpWydjU0oprphcKFouXYcuhKsEjynPAkf2QO3baTuUyOj4hyvwJidth0EtFIrVYkT1Cmv/GXP+Ss1WEc7QKzDJxPzqNiGgerorpYRiTBeLtkOIPZFKtbS6O0cuRC3k3rm3aNm0cBIwg5vg7ByyF5Rlqrsb8tB+22HEXqvbhjC5EAXIlGehL38O796D6nNq326LEQXPezEBoTWXeLe5licXtbG3WpKAqFGqrw/i2EHbYfhu4SY3UYxZ33rtQbyYAoy2HQm1UMuTi7tnKyRXhdA6eSMjMBeu2A6jpdz7D9Y8Rn/1GJzt2wKIJhju8MNQ7CKn1mn5DX3xwaWaNzqJqH7OxZvwZoJprU3kB95zIYoAPTlZ90q5sE9Dq74+2yFQAJhciNqMGthkO4RVFU7vsB0CBYDJhajNzFUUACodYJ2tWwAAqrcHat9uyEzGVmgAgPRf1e7rRO2FyYWojeliEe6D4coXQmD89Y2Q+ZWrSqvu7kWNs/wW9ik78g+TC1FMeM9Gkf/+hzWbYwnHAYRA8Uu7IZIt3H+i2Z46LphciOLE1H5zN64LGIP0Tz6FPvFq9XmZyVRGGz6NZsJavof8x+RCRFWmVFrUL0c4DmRuQ/W+zWqEs/bOBplONxMeRQiTCxGtyJuYgPdsdP6+TQ3CcSASSaiB/lXPJRwH+sgev0OkkLKfXFp485CIfLLCdBoAGM+DKc/WTED6K0erNcSM6wIf/bxlIVK4hCC52A+BlpCKSZ/qt0riUR9fhTm+n6vEYsj+O7sOpjiMOHkYMrfyEkxaQHurvmEQ1cuUShDnPlu0SIDiwX5yCYh6NAYzy5UqdeGohXxkXHfRIgGKh5YnF3lof2VjlmXuw8ctb47TNjhVSURNanlVZHPtFrRbnn9CqsCmwhaxcc2oitLPytbvE0We6t8E09cDvSEJnP3Udjhtp+UfUU15dvH8fZ0NglRXZ+WNw0eqtwdqY6+v5yTL2HCK1mt6BvLFFHRC8X5sC7R85LJMHTeKheMAg5sgZ8vQxaJvl/ZGx3w7F4UEFx5QA4TjYPpXX0fHjy7Cm5gAJiYg7z8AP6L4L/jkUgfjuvCu3rAdBhG1GeO6yP7j5/BYhqblwnXnVgg4O4dsR0FUF5FI+j51S63nTUzYDiEWwpVcjIF75z6XwlIkyA1ZyA7WyiKqJXzTYgGs/FFdndBThUo5CqJ18sbHbYdAFFrhGrkEhT0liIhaKnwjlwBwzpWIqLXiOXIhWqd6epYQEZMLgMVvGFO/dcZiJFSLzGRCs8iD9+mI6sPkgsVvGLk/P2cxEqrF7NvR2r7uROS7SCWXqbdOt3xawrgunMEB7l8IEXPhCouOLuDsHArNSI5oJZFKLhve/jCQaQmT7YCQK//xLu0DLjMZJiMKhEyn4d66w7I3FHqRSi5B8W7eXp7EFiQP79i+Rd8qn9qPwvdOBBEaxdzS3z2isGJyqdeCzZ3ig0uLvqXe+QSpcRfum8cx/Run5r8hBNTuHfNfOg5kJgN5aP+aZW6e/uGXFj8hFZwd29cbPbWJpb97RGHFdZU+cX56vvLfJc+XBzohb6IyR64URCoFc3cY2lteicAZegXunXsAgIF/HINxHEBImPIshFJw+zuB2y3+P0JE5AM7yUWIeMwZGwP53sXqY1MqwVvlxrQ3/Lj6WI4+h9m3C7ojAZy7XOmLs1pDo6U/07mv4/KzJqJQsTItpnp7WrfaRYjITh+ZBWXA3UeP4V25BnPucp0vriQQ1dtT+fnOJZQ6Eovq7YF6dQ8K/+J0wzFTfIhEkptIqW527rls6oVQLVpdZQzc23dbc+4I8EbH6m+KNpfgN/VC37yL9LPy6sdTPEkFSAXZkeZ+I6qblY8h3mfXbVyWlno5qplrzKbe+cRiMBRWqrsTGOiDd+Wa7VBaQnV3V9tle89fWI6mfXCMS0Sr8kbHgHZpES4VhFKLpqDN9DQgm5/EUb09bKW+AJciE9HKIl4JQCQWT+PJdAoy27HoOT0zA10sQheLTV3LbO1v6vXthsmFiGqTCs4rW21HsW6qtweP/+AEVD5ffU4Xiy2b+tKXrtaOI5+HenVPS64ZZkwuFCoilXr5INqfmCNPKhS+dwJ6ZNR2JOvmjY6h/z+/b71/k/E8iJnZ2P1OM7mQVeL4wfnSOlJBdXdVHnNvjlUymUD+nZtNTxXFwhpJQxcKlRWsMfud9i+5xCwr0/qJRBLOtsp0i/j8znxpHe3BffzEXmBUpWdm4D2L7qglKGrPTuD0YdthhJJ/yeVlVjZfPspEQ2sqHhiA6uqELhRsh0LUEJlOw9myGVNvnYZ349bqlTNizPdpMfH+pUrZk1wOqqvT79NTGzDlWST/7jz3FFCVs2Wz7RDqJrYOwhvsQf7GpO1QQs3/fS4vRzC6UFy1JwrFXMzmn2l1K03BqYP77G/eXFKfz7tZqR7L3+DVte6GvvZC2W+ctZGI7BOOg+L35mvZrdRpVKct/b0unNrnB6F1id1qMTXAjU5ENuivHqt2cTWui+z/Pb/ma8z5K60Oa4ULM6E0y5fk4gy94sdpWmZhfO6DYYuRELU/Z3Cg5qKe5I2H0LPzxVHDOLNB/vFlzDnX4KqVxILGWY0KIj4iqjBLyqvM4TLzeInMtJjcuR3iwC7bYRDRGrybtzmtRNGpiuxd/8J2CEQUEaq7GwDgjY9bjiS+IpNciChAUs1XToggJhX7IjMtRkQBOHMEwnHgbIvOpkYKJyYXIqpyHj+H8Ty4d+/bDqVKZjK2Q6B1YHIhoir3zr3Q3YwX2SzrFUYQ77kQUah5IyO2Q6B14MglJEQqxU9nRNQ2mFxCQuY2QChlOwwiIl9wWiwk2JiJiNpJW4xcnG1b51vlEhGRdW2RXNz7DyK94YuoFnHsoO0Q1qQO7IXMZm2HQSHEaTGiEBGOA9HRAdnVCX39TugaUolUCqq7q1qE0tx5AD09bTkqCiMmF6Kw0boyGg8hUyotqm6si0WL0VCYtcW0WLty3zzOzpkxY1wXulCwHQZR05hcQiz183swHu8lEVH0MLmEmDcysq5SHOaN17ghk6rky/IpIpGsdIkkCgCTS0g527ZCvbqnodeovbsg02mIDy+Hrj4U2TP1zUMQJw5BbR2E++ix7XBoAbV7R9uutmub5KI29kJms5j59inIXM52OE3To2PAo6eNvejxSKVHeciXZYtEEs4WlnRvKSEq+78AZP/6IuSNe9BPn1kOihZytm0Fno7ClEq2Q2kJYUx9H3G/Id9qdSzRc+ow8NHP/TlXFJszNRvzmSPA2U/9iydmVP8mIJettBUGovk7RJH0Y/32mse0zcjFBvHJVX/Ok0hC9fX6cq4gOYP9Td3bEec+8zGa5onjB6HyedthrEn19VUebOyGd+te9Xmnv4+VKkJO5fMQx8O/OdYPTC5NMK7rz3nKs/CeNDgFZpEz9ApkNgt3+GFT93b8+vn5QSSSgAb09IztUFblbN0CbOqpfPHk2aKRivvoMUcuIaenZ6AexqOOIJMLNay8padSxfn4wbbZhyPSKQCVRB9m7oNheFeuAWCx0ygy5dnYLKpoj3cGCpT42UW4AMToeKhGH83Qk5PAhSu2wyBqGxy50LqF/VN+WOmvHquu5CJqVxy5ELXKCqu35D9dQHuM94hWxuRC1CLywB6I0izEbBnu3fu2wyEKFKfFiHxS+M3Ti77Wlz8Hxp7DFMO9Ao2oFZhcaF3EiUPcU/GSPLQfIpVC9v98uOx73uhYpUYcUcwwudC6GEei/OZR22GEw807MLNc3EC0EO+5xIDq6gS0gTGmsuTWD2c/RcKfM0WenuG0F9FSTC4x4O19BaKsIWdd4OUGPCKiVmJyiQFVmAWGH8N7/qL118rn4U1MtPw6RBRuTC5tTJw8DC/jAO9eCO6ikk3KiIg39NuOM9CPkX//BiAVzLnLkP94MdDrBzE6IqLw48il3XSkoRMvRw/sRklElnDk0ob6/8v7TZVeF6kUVG8P5KH9PkZFRHHCkUubcW/fbfoccvcQMFuG5soyIlonJheqKn3rJMoZiQ1vL99pTkTUCE6LUaULIwDhgYmFiHzB5EIQB3ZBpFJI/u3HtkMhojbBaTGCvnTVdghWzbVqbpeumkRhwJELxZ5IJiEcByKVgjh52HY4RG2ByYViS/X2AAB0sQg9MwP3jYNQzyYgMxnLkRFFH6fFKLbM9OJqxuqdT+A5DiD4mYuoWfwrCoG5OX9x/KDlSOJFF4vLnjOuC1NmbxaiZjG5hEDxW68DQgCXb9oOhYjIF5wWs8TZvg3exjwm9uSQ+/5ZAIAplSxHRUTkD45cAjY3Bfb81GaYTz6rJpbmTio4pUZEocLkErDxf3USqru7shPep6rF+heOwutg02EiCg9OiwXAGRyA+2QE0B66/ucHWH+94nkym63cfC6VIINsBlaDs30bypt7ID64ZDUOIgoPjlwC4D563FQJfACAEFB9fQAA/YvHYF7dAbWpz4fomufevc/EQtQCqq+vstgngjhyCSHV1wc9MbH8Bv/L8iSJT+/Ae/4cbpiagQkBZ6C/kkhXoQ7shZES+vLnAQVGFF16fDyyTf+YXEJIqPkBpUynIXZsA1wP5vEIAMAbH7cV2sqEBJRa8zDvs+sBBEPUHqJc747TYiHkPn4CUypBJJIQu7Zj9ORGeDdvQ09O2g5tZdqD+2C48lgqqAN77cZDRFZx5BJipjwL78o1dF2xHUmDtAc8WH16bKHid08jf+kJ3Ft3WhcTEQWKyYVawpuYqPvYzF98iOgO/omoFk6LERGR75hc6iSzWajdO2yHQUQUCUwudZDpNMa/exjuprztUIiIIoH3XNYgHAfTXzuMrrcvsLAkEVGdOHJZQu3bjZlvn6p+bVwXqb/5mImFiKgBHLks4V27ifQ121EQEUUbRy4UOjKbhTPQbzsMImoCk8tLzuBAZAvEtRvjujAzM2sfSEShxeTykvvocWQLxLUbUypBTxUgj+y3HQoRrVNsk4v3S6/7ej559ICv54s747ow127bDoOI1im2ycWZ9Hf1lyhyNZnfuEKPKLpim1zMeX+rQXrXv2joeNXbA/PGa77GQEQUFlyKbIk3/gLqchnadiBERC0Q25GLddoLd38WIqImMLkQEZHvmFyIiMh3TC5EK1B7dsIZegVApTK26u0BhMD0d06t8UoiYnIhWsH0zh64d+9Xvti3A8+/vhcylULH/ztvNzBqmP7qMUAqqO5uOFs22w4nFiK7Wkwkknj0hyew5W+eNLwMmKgeyR+dqz7Wl64idwlc3RdBat9uuAkJqT3oqQLE9LTtkGIhcslFptOQXZ0w3Xls+mSaiYWIVuVduwnnZaVzU56FKduNJy4iNy2mj+4FAHif34T8pwsrHsd5caJ4U3t32Q4h1iKXXHD2U7iPn6xZZHLDlZGAAqJAScUPDlSfkTHbEcRa6JOLzGQgX3u14dd5N1n0sC1pDx0/+Mh2FBQB3vi47RBiTRjDOvNEROSv0I9ciIgoephciIjId0wuRETkOyYXIiLyHZMLERH5jsmFiIh8x+RCRES+Y3IhIiLfMbkQEZHv/j9asBthtPAlyQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ani = tonic.utils.plot_animation(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1badfd5-35ed-43a3-8f1e-bbecefb0a08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.3 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TimeitResult : 28.3 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit -o -r 1 tonic_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a8ae5-c6ce-4933-8b08-702b05071923",
   "metadata": {},
   "source": [
    "157 ms ± 4.12 ms per loop (mean ± std. dev. of 10 runs, 10 loops each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "808a1540-0251-41ee-8c5e-1157c5fa25e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     events, target \u001b[38;5;241m=\u001b[39m tonic_dataset[i]\n\u001b[1;32m     12\u001b[0m     targets\u001b[38;5;241m.\u001b[39mappend(target)\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_events\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m np\u001b[38;5;241m.\u001b[39msave(cache_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/targets\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39masarray(targets))\n\u001b[1;32m     16\u001b[0m np\u001b[38;5;241m.\u001b[39msave(cache_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tonic_dataset))\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/snn-gpu/lib/python3.10/site-packages/numpy/lib/npyio.py:522\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[1;32m    521\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n\u001b[0;32m--> 522\u001b[0m     \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfix_imports\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfix_imports\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/snn-gpu/lib/python3.10/site-packages/numpy/lib/format.py:723\u001b[0m, in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _isfileobj(fp):\n\u001b[0;32m--> 723\u001b[0m         \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtofile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39mnditer(\n\u001b[1;32m    726\u001b[0m                 array, flags\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexternal_loop\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuffered\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzerosize_ok\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    727\u001b[0m                 buffersize\u001b[38;5;241m=\u001b[39mbuffersize, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cache_dir = \"./data/EVIMO/cache\"\n",
    "targets = []\n",
    "num_ones = 0\n",
    "\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "for i in range(len(tonic_dataset)):\n",
    "    events, target = tonic_dataset[i]\n",
    "    targets.append(target)\n",
    "    np.save(cache_dir+f\"/{i}_events\", events)\n",
    "\n",
    "np.save(cache_dir+f\"/targets\", np.asarray(targets))\n",
    "np.save(cache_dir+f\"/length\", len(tonic_dataset))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3f6b203-79d9-4fcf-9975-8fe6c65f9754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedEVIMODataset(Dataset):\n",
    "    def __init__(self, \n",
    "                dir: str):\n",
    "        self.dir = dir\n",
    "        \n",
    "        self.length = np.load(self.dir+f\"/length.npy\")\n",
    "        self.targets = np.load(self.dir+f\"/targets.npy\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        events = np.load(self.dir+f\"/{i}_events.npy\")\n",
    "        return events, self.targets[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1afbae57-e797-4e98-a65e-0edfb6d4b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CachedEVIMODataset(dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8b6c02d-48ab-4b6c-9fe4-6573cd6664ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.27 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 100 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TimeitResult : 6.27 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 100 loops each)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit -o -r 1 dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a501fa48-6e0d-42fe-baad-7dc1538ec132",
   "metadata": {},
   "source": [
    "20.7 ms ± 1.09 ms per loop (mean ± std. dev. of 10 runs, 10 loops each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea1e86b-eb3e-4f98-aadb-00494f9e96ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "events, target = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbce7f80-4ba0-4581-8030-f73ec86e946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0c83d3-3968-43b8-b762-7de17e76ac1a",
   "metadata": {},
   "source": [
    "num_ones = 0\n",
    "#targ_set = set()\n",
    "\n",
    "for i in range(0, len(dataset)):\n",
    "    _, target = dataset[i]\n",
    "    #print(target.argmax())\n",
    "    if target.argmax() == 1:\n",
    "        num_ones += 1\n",
    "        #targ_set.add(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd06b2-9fd0-4035-821e-62b5a88e5ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ones / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7fa9a80-586b-4ace-b999-c9d0cd987723",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(tonic_dataset, batch_size=batch_size, shuffle=False) # collate_fn=tonic.collation.PadTensors(),\n",
    "#test_trainloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7fa7d1e-a4e4-461a-90c3-4b663b07f1bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for i_batch, sample_batched in enumerate(trainloader):\n",
    "#    print(i_batch, sample_batched['data'].size(), sample_batched['target'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f582f111-a1af-4ee6-adeb-b4bd1f13f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "datait = iter(trainloader)\n",
    "bins, targets = next(datait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45aaaff3-d489-42d5-9ecb-0489e2a53c0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 1, 480, 640])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "201836b1-fd13-4bd2-aea0-371333a58b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "321a0824-3a31-4af3-b745-e4c5f7d5eee1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER SIZE: 1210944\n",
      "STDV: 0.000454368247933539\n",
      "LAYER SIZE: 34048\n",
      "STDV: 0.002709718654762875\n",
      "LAYER SIZE: 256\n",
      "STDV: 0.03125\n"
     ]
    }
   ],
   "source": [
    "data, target = next(iter(trainloader))\n",
    "\n",
    "loss = torch.nn.SmoothL1Loss()\n",
    "\n",
    "def decolle_loss(r, s, tgt):\n",
    "    loss_tv = 0\n",
    "    for i in range(len(r)):\n",
    "        if r[i].shape != tgt.shape:\n",
    "            print(f\"Loss Readout shape : {r[i].shape}\")\n",
    "            print(f\"Loss Target shape : {tgt.shape}\")\n",
    "        loss_tv += loss(r[i],tgt) \n",
    "    return loss_tv\n",
    "\n",
    "\n",
    "convnet_sg = lenet_decolle_model.LenetDECOLLE(out_channels=2,\n",
    "                    Nhid=[64, 128], #Number of convolution channels\n",
    "                    Mhid=[256],\n",
    "                    kernel_size=[8, 16],\n",
    "                    pool_size=[4, 8],\n",
    "                    input_shape=[1, 480, 640],  # data.shape[1:],\n",
    "                    alpha=[.95],\n",
    "                    alpharp=[.65],\n",
    "                    beta=[.92],\n",
    "                    num_conv_layers=2,\n",
    "                    num_mlp_layers=1,\n",
    "                    lc_ampl=0.5).to(device)\n",
    "\n",
    "#net = lenet_decolle_model.LenetDECOLLE(Nhid=[1,8],Mhid=[32,64],out_channels=2, input_shape=[2, 480, 640]).to(device)\n",
    "\n",
    "#convnet_sg\n",
    "\n",
    "data_d = data.to(device)\n",
    "target_d = target.to(device)\n",
    "convnet_sg.init_parameters(data_d) # Modifies readout dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e26c9fd9-1b04-4d7d-b4fc-a059bac83ce8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 1, 480, 640])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7408e907-b4a4-4eca-8a5d-e3943bd68333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98c9479b-0c6c-4982-8def-9dd7455f1891",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a1d6b423de4c12ad8a08abfbf0eb3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.3657,  0.1922],\n",
      "        [-0.1385, -0.1003],\n",
      "        [-0.1654, -0.2374],\n",
      "        [-0.0819, -0.1092]], device='cuda:0', grad_fn=<AddmmBackward0>), tensor([[-0.1582,  1.2147],\n",
      "        [ 0.1850,  1.2803],\n",
      "        [-0.2844,  1.2766],\n",
      "        [ 0.1006,  0.9054]], device='cuda:0', grad_fn=<AddmmBackward0>), tensor([[0.3028, 0.4670],\n",
      "        [0.2242, 0.4168],\n",
      "        [0.1537, 0.4155],\n",
      "        [0.2442, 0.3245]], device='cuda:0', grad_fn=<AddmmBackward0>)]\n",
      "tensor([[0.3028, 0.4670],\n",
      "        [0.2242, 0.4168],\n",
      "        [0.1537, 0.4155],\n",
      "        [0.2442, 0.3245]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3028, 0.4670],\n",
      "        [0.2242, 0.4168],\n",
      "        [0.1537, 0.4155],\n",
      "        [0.2442, 0.3245]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[tensor([[-0.1044,  0.1879],\n",
      "        [-0.1296,  0.1187],\n",
      "        [-0.3328, -0.1734],\n",
      "        [ 0.0996, -0.3822]], device='cuda:0', grad_fn=<AddmmBackward0>), tensor([[ 0.3466, -0.4593],\n",
      "        [ 0.3427, -0.2044],\n",
      "        [ 0.0792, -0.1132],\n",
      "        [ 0.2389, -0.2165]], device='cuda:0', grad_fn=<AddmmBackward0>), tensor([[-0.2355,  1.1238],\n",
      "        [-0.1997,  1.0816],\n",
      "        [-0.3050,  1.1006],\n",
      "        [-0.3032,  1.1661]], device='cuda:0', grad_fn=<AddmmBackward0>)]\n",
      "tensor([[-0.2355,  1.1238],\n",
      "        [-0.1997,  1.0816],\n",
      "        [-0.3050,  1.1006],\n",
      "        [-0.3032,  1.1661]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.0674,  1.5908],\n",
      "        [ 0.0246,  1.4984],\n",
      "        [-0.1513,  1.5161],\n",
      "        [-0.0590,  1.4906]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[tensor([[-0.2129, -0.0864],\n",
      "        [-0.1579,  0.1256],\n",
      "        [-0.3099, -0.0523],\n",
      "        [-0.2689, -0.0630]], device='cuda:0', grad_fn=<AddmmBackward0>), tensor([[-0.0693,  1.8238],\n",
      "        [ 0.3419,  1.2516],\n",
      "        [-0.3809,  1.8111],\n",
      "        [-0.2253,  1.9280]], device='cuda:0', grad_fn=<AddmmBackward0>), tensor([[0.0582, 0.2540],\n",
      "        [0.2276, 0.2962],\n",
      "        [0.1144, 0.3682],\n",
      "        [0.2053, 0.3313]], device='cuda:0', grad_fn=<AddmmBackward0>)]\n",
      "tensor([[0.0582, 0.2540],\n",
      "        [0.2276, 0.2962],\n",
      "        [0.1144, 0.3682],\n",
      "        [0.2053, 0.3313]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.1256,  1.8448],\n",
      "        [ 0.2522,  1.7946],\n",
      "        [-0.0369,  1.8843],\n",
      "        [ 0.1463,  1.8219]], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m opt_conv\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     33\u001b[0m loss_hist \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_tv\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(rt[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     36\u001b[0m readout \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rt[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/snn-gpu/lib/python3.10/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/snn-gpu/lib/python3.10/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/snn-gpu/lib/python3.10/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/miniforge3/envs/snn-gpu/lib/python3.10/site-packages/torch/_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m~/miniforge3/envs/snn-gpu/lib/python3.10/site-packages/torch/_tensor_str.py:137\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_select(\n\u001b[1;32m    138\u001b[0m         tensor_view, torch\u001b[38;5;241m.\u001b[39misfinite(tensor_view) \u001b[38;5;241m&\u001b[39m tensor_view\u001b[38;5;241m.\u001b[39mne(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "opt_conv = torch.optim.Adamax(convnet_sg.get_trainable_parameters(), lr=1e-9, betas=[0., .95])\n",
    "for e in range(epochs):        \n",
    "    error = []\n",
    "    accuracy=[]\n",
    "    for data, label in tqdm(iter(trainloader), desc=f\"Epoch {e}\"):\n",
    "        convnet_sg.train()\n",
    "        loss_hist = 0\n",
    "        data_d = data.to(device, dtype=torch.float)\n",
    "        label_d = label.to(device)\n",
    "        convnet_sg.init(data_d, burnin=10)\n",
    "        readout = 0\n",
    "\n",
    "        #print(label_d)\n",
    "        #print(label_d.shape)\n",
    "        #break\n",
    "\n",
    "        data_d = data_d.transpose(0, 1)\n",
    "        \n",
    "        for n in range(num_bins_per_frame):\n",
    "           # print(f\"Data shape: {data_d[n].shape}, overall: {data_d.shape}\")\n",
    "            st, rt, ut = convnet_sg.forward(data_d[n])\n",
    "            #print(\"Readout\")\n",
    "            #print(len(rt), rt[0].shape, rt[1].shape, rt[2].shape)\n",
    "            #print(f\"Label: {label_d[n].shape} overall {label_d.shape}\")\n",
    "            #print(label_d[n].shape)\n",
    "            #print(label_d[n])\n",
    "            loss_tv = decolle_loss(rt, st, label_d)\n",
    "            loss_tv.backward()\n",
    "            opt_conv.step()\n",
    "            opt_conv.zero_grad()\n",
    "            loss_hist += loss_tv\n",
    "            print(rt)\n",
    "            print(rt[-1])\n",
    "            readout += rt[-1]\n",
    "            print(readout)\n",
    "        error += (readout.argmax(axis=1)!=label_d.argmax(axis=1)).float()\n",
    "        accuracy+=(readout.argmax(axis=1)==label_d.argmax(axis=1)).float()\n",
    "    print('Training Error', torch.mean(torch.Tensor(error)).data)\n",
    "    print('Training accuracy', torch.mean(torch.Tensor(accuracy)).data)     \n",
    "    print('Epoch', e, 'Loss', loss_hist.data)\n",
    "    PATH = './mnist_network_sg_conv.pth'\n",
    "    torch.save(convnet_sg.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e62e65-490a-4c02-b2ac-f5070f4280fe",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "opt_conv = torch.optim.Adamax(convnet_sg.get_trainable_parameters(), lr=1e-6, betas=[0., .95])\n",
    "\n",
    "# Their data_d is T, batchsize, pol, x, y (16, 4, 1, 28, 28)\n",
    "# Mine is batchsize, T, pol, x,y\n",
    "\n",
    "for e in range(epochs):        \n",
    "    error = []\n",
    "    accuracy=[]\n",
    "    for data, label in tqdm(iter(trainloader), desc=f\"Epoch {e}\"):\n",
    "\n",
    "        #if label.shape[0] != 4:\n",
    "            #print(f\"last entry\")\n",
    "            #break\n",
    "        \n",
    "        convnet_sg.train()\n",
    "        loss_hist = 0\n",
    "        data_d = data.to(device)\n",
    "        label_d = label.to(device)\n",
    "        convnet_sg.init(data_d, burnin=10)\n",
    "        readout = 0\n",
    "\n",
    "        data_d = data_d.transpose(0, 1)\n",
    "        \n",
    "        #print(label_d.shape)\n",
    "        #print(label_d)\n",
    "        \n",
    "        for n in range(num_bins_per_frame):\n",
    "            # CORRECT: Data shape: torch.Size([4, 2, 480, 640]), overall: torch.Size([8, 4, 2, 480, 640])\n",
    "            #print(f\"Data shape: {data_d[n].shape}, overall: {data_d.shape}\")\n",
    "            #print(data_d[n])\n",
    "            \n",
    "            st, rt, ut = convnet_sg.forward(data_d[n])\n",
    "            #print(\"Readout\")\n",
    "            #print(len(rt), rt[0].shape, rt[1].shape, rt[2].shape)\n",
    "            #print(rt[-1].shape)\n",
    "\n",
    "            # CORRECT: Label shape: torch.Size([4, 2]), overall: torch.Size([8, 4, 2])\n",
    "            #print(f\"Label shape: {label_d[n].shape}, overall: {label_d.shape}\")\n",
    "            #print(label_d[n].shape, label_d[n])\n",
    "            \n",
    "            #print(label_d)\n",
    "            #break\n",
    "            loss_tv = decolle_loss(rt, st, label_d)\n",
    "            \n",
    "            loss_tv.backward()\n",
    "            opt_conv.step()\n",
    "            opt_conv.zero_grad()\n",
    "            loss_hist += loss_tv\n",
    "            readout += rt[-1]\n",
    "\n",
    "        #print(f\"Readout: {readout}\")\n",
    "        #error += (readout.argmax(axis=1)!=label_d.argmax(axis=1)).float()\n",
    "        #accuracy+=(readout.argmax(axis=1)==label_d.argmax(axis=1)).float()\n",
    "\n",
    "        # SEEM CORRECT\n",
    "        argmaxed_readout = readout.argmax(axis=1)\n",
    "        #print(\"argmaxeds\")\n",
    "        #print(argmaxed_readout)\n",
    "        #print(label_d)\n",
    "        #print(label_d[-1])\n",
    "        #print(label_d.argmax(axis=1))\n",
    "        argmaxed_label = label_d.argmax(axis=1)\n",
    "        #print(argmaxed_label)\n",
    "        #break\n",
    "        #print(\"argmaxed readout\")\n",
    "        #print(argmaxed_readout)\n",
    "        #print(\"argmaxed label\")\n",
    "        #print(argmaxed_label)\n",
    "        error += (argmaxed_readout!=argmaxed_label).float()\n",
    "        accuracy+=(argmaxed_readout==argmaxed_label).float()\n",
    "        #print(f\"Accuracy: {accuracy}\")\n",
    "        #break\n",
    "        \n",
    "    print('Training Error', torch.mean(torch.Tensor(error)).data)\n",
    "    print('Training accuracy', torch.mean(torch.Tensor(accuracy)).data)     \n",
    "    print('Epoch', e, 'Loss', loss_hist.data)\n",
    "    PATH = './EVIMO_class_cnn.pth'  \n",
    "    torch.save(convnet_sg.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9961fd94-fc2f-41db-aab0-25a5539903c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(loss_tv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ee114-ef61-4a57-9d90-f28f99147570",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "error = []\n",
    "accuracy=[]\n",
    "y_pred = []\n",
    "y_true = []\n",
    "#convnet_sg.requires_init = True\n",
    "for item in iter(batched_trainloader):\n",
    "    #print(\"Item\")\n",
    "    #print(item)\n",
    "    data = item['data']\n",
    "    label = item['target']\n",
    "    if label.shape[0] != 4:\n",
    "        #print(f\"last entry: {label}\")\n",
    "        break\n",
    "\n",
    "    #print(\"earliest label\")\n",
    "    #print(label)\n",
    "    #print(label.shape)\n",
    "\n",
    "    \n",
    "    loss_hist = 0\n",
    "    data_d = data.to(device)\n",
    "    label_d = label.to(device)\n",
    "    print(f\"data_d shape: {data_d.shape}\")\n",
    "    print(f\"data_d_tranpose shape: {data_d.transpose(0,1).shape}\")\n",
    "    print(f\"Len convnet: {len(convnet_sg)}\")\n",
    "    print(f\"data init shape: {data_d.transpose(0,1)[:, 0, :, :].shape}\")\n",
    "    print(f\"Test data shape: {data_d[0].shape}\")\n",
    "    \n",
    "    convnet_sg.init_evimo(data_d)\n",
    "    readout = 0\n",
    "    with torch.no_grad():\n",
    "        #for i in range(0, len(data_d)):\n",
    "        #d = data_d[i:i+1]\n",
    "        \n",
    "        #print(d.shape)\n",
    "        \n",
    "        st, rt, ut = convnet_sg.forward(data_d)\n",
    "\n",
    "        \n",
    "        \n",
    "        print(f\"Label shape: {label_d.shape}\")\n",
    "        print(\"Readout\")\n",
    "        print(rt)\n",
    "        print(rt[0].shape)\n",
    "\n",
    "        #print(\"labels\")\n",
    "        #print(label_d)\n",
    "        #print(label_d.shape)\n",
    "        \n",
    "        \n",
    "        loss_tv = decolle_loss(rt, st, label_d)\n",
    "        \n",
    "        \n",
    "        loss_hist += loss_tv\n",
    "        readout += rt[-1]\n",
    "        print(f\"Readout: {readout}\")\n",
    "        output = (readout.argmax(axis=1)).data.cpu().numpy()\n",
    "        print(f\"Output: {output}\")\n",
    "        y_pred.extend(output)\n",
    "        #labels = label_d.argmax(axis=1)\n",
    "        print(f\"Labels: {label_d}\")\n",
    "        labels = (label_d.argmax(axis=1)).data.cpu().numpy()\n",
    "        print(f\"Labels: {labels}\")\n",
    "        y_true.extend(labels)\n",
    "        accuracy+=(readout.argmax(axis=1)==label_d.argmax(axis=1)).float()\n",
    "        error += (readout.argmax(axis=1)!=label_d.argmax(axis=1)).float()\n",
    "        break\n",
    "        \n",
    "print('Testing Error', torch.mean(torch.Tensor(error)).data)\n",
    "print('Testing accuracy', torch.mean(torch.Tensor(accuracy)).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14e8c82-256e-490b-b93b-7f5ac7d7493d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(y_true)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2bc5a-e41a-4848-b296-e23b3c12b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "convnet_sg = lenet_decolle_model.LenetDECOLLE( out_channels=2,\n",
    "                    Nhid=[16,32], #Number of convolution channels\n",
    "                    Mhid=[64],\n",
    "                    kernel_size=[7],\n",
    "                    pool_size=[2,2],\n",
    "                    input_shape=[2, 480, 640],  # data.shape[1:],\n",
    "                    alpha=[.95],\n",
    "                    alpharp=[.65],\n",
    "                    beta=[.92],\n",
    "                    num_conv_layers=2,\n",
    "                    num_mlp_layers=1,\n",
    "                    lc_ampl=.5).to(device)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685f938a-2daa-4af3-98e2-ddf8be8f91b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = lenet_decolle_model.LenetDECOLLE(Nhid=[16,32],Mhid=[64],out_channels=2, pool_size=[2,2], num_conv_layers=2, num_mlp_layers=1, lc_ampl=.5, input_shape=[2, 480, 640])\n",
    "d = torch.zeros([1, 2, 480, 640])\n",
    "print(d.shape)\n",
    "st, rt, ut = net.forward(d)\n",
    "print(rt[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f5490-a0b7-4311-80af-37368d824a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec08fa76-8a93-42a9-91b8-05eca9db7f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e3376-231b-4b64-a521-57b2ca6944f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.zeros([1, 2])\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e406213-5c03-4e68-bc2f-e8fe4a8c4e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tv = decolle_loss(rt, st, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bffc1a-f0f1-44b2-b386-620613fa93ab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79139852-ca64-4c41-8367-55d7334f4a94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
